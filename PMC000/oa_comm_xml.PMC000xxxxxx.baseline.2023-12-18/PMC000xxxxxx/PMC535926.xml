<!DOCTYPE article PUBLIC "-//NLM//DTD Journal Archiving and Interchange DTD v2.3 20070202//EN" "archivearticle.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id><journal-title>BMC Bioinformatics</journal-title><issn pub-type="epub">1471-2105</issn><publisher><publisher-name>BioMed Central</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">15555081</article-id><article-id pub-id-type="pmc">PMC535926</article-id><article-id pub-id-type="publisher-id">1471-2105-5-181</article-id><article-id pub-id-type="doi">10.1186/1471-2105-5-181</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group></article-categories><title-group><article-title>Discriminative topological features reveal biological network mechanisms</article-title></title-group><contrib-group><contrib id="A1" corresp="yes" contrib-type="author"><name><surname>Middendorf</surname><given-names>Manuel</given-names></name><xref ref-type="aff" rid="I1">1</xref><email>mjm2007@columbia.edu</email></contrib><contrib id="A2" contrib-type="author"><name><surname>Ziv</surname><given-names>Etay</given-names></name><xref ref-type="aff" rid="I2">2</xref><email>ez87@columbia.edu</email></contrib><contrib id="A3" contrib-type="author"><name><surname>Adams</surname><given-names>Carter</given-names></name><xref ref-type="aff" rid="I3">3</xref><email>cca2001@columbia.edu</email></contrib><contrib id="A4" contrib-type="author"><name><surname>Hom</surname><given-names>Jen</given-names></name><xref ref-type="aff" rid="I4">4</xref><email>jch149@columbia.edu</email></contrib><contrib id="A5" contrib-type="author"><name><surname>Koytcheff</surname><given-names>Robin</given-names></name><xref ref-type="aff" rid="I4">4</xref><email>rmk2002@columbia.edu</email></contrib><contrib id="A6" contrib-type="author"><name><surname>Levovitz</surname><given-names>Chaya</given-names></name><xref ref-type="aff" rid="I5">5</xref><email>cl2021@barnard.edu</email></contrib><contrib id="A7" contrib-type="author"><name><surname>Woods</surname><given-names>Gregory</given-names></name><xref ref-type="aff" rid="I3">3</xref><email>gaw59@columbia.edu</email></contrib><contrib id="A8" contrib-type="author"><name><surname>Chen</surname><given-names>Linda</given-names></name><xref ref-type="aff" rid="I6">6</xref><email>lchen@math.columbia.edu</email></contrib><contrib id="A9" contrib-type="author"><name><surname>Wiggins</surname><given-names>Chris</given-names></name><xref ref-type="aff" rid="I7">7</xref><xref ref-type="aff" rid="I8">8</xref><email>chris.wiggins@columbia.edu</email></contrib></contrib-group><aff id="I1"><label>1</label>Department of Physics, Columbia University, New York, USA</aff><aff id="I2"><label>2</label>College of Physicians and Surgeons, Columbia University, New York, USA</aff><aff id="I3"><label>3</label>Columbia College, Columbia University, New York, USA</aff><aff id="I4"><label>4</label>Fu Foundation School of Engineering and Applied Sciences, Columbia University, New York, USA</aff><aff id="I5"><label>5</label>Barnard College, Columbia University, New York, USA</aff><aff id="I6"><label>6</label>Department of Mathematics, Columbia University, New York, USA</aff><aff id="I7"><label>7</label>Department of Applied Physics and Applied Mathematics, Columbia University, New York, USA</aff><aff id="I8"><label>8</label>Center for Computational Biology and Bioinformatics, Columbia University, New York, USA</aff><pub-date pub-type="collection"><year>2004</year></pub-date><pub-date pub-type="epub"><day>22</day><month>11</month><year>2004</year></pub-date><volume>5</volume><fpage>181</fpage><lpage>181</lpage><ext-link ext-link-type="uri" xlink:href="http://www.biomedcentral.com/1471-2105/5/181"/><history><date date-type="received"><day>24</day><month>7</month><year>2004</year></date><date date-type="accepted"><day>22</day><month>11</month><year>2004</year></date></history><copyright-statement>Copyright &#x000a9; 2004 Middendorf et al; licensee BioMed Central Ltd.</copyright-statement><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/2.0"><p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/2.0"/>), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</p></license><abstract><sec><title>Background</title><p>Recent genomic and bioinformatic advances have motivated the development of numerous network models intending to describe graphs of biological, technological, and sociological origin. In most cases the success of a model has been evaluated by how well it reproduces a few key features of the real-world data, such as degree distributions, mean geodesic lengths, and clustering coefficients. Often pairs of models can reproduce these features with indistinguishable fidelity despite being generated by vastly different mechanisms. In such cases, these few target features are insufficient to distinguish which of the different models best describes real world networks of interest; moreover, it is not clear a priori that <italic>any </italic>of the presently-existing algorithms for network generation offers a predictive description of the networks inspiring them.</p></sec><sec><title>Results</title><p>We present a method to assess systematically which of a set of proposed network generation algorithms gives the most accurate description of a given biological network. To derive discriminative classifiers, we construct a mapping from the set of all graphs to a high-dimensional (in principle infinite-dimensional) "word space". This map defines an input space for classification schemes which allow us to state unambiguously which models are most descriptive of a given network of interest. Our training sets include networks generated from 17 models either drawn from the literature or introduced in this work. We show that different duplication-mutation schemes best describe the <italic>E. coli </italic>genetic network, the <italic>S. cerevisiae </italic>protein interaction network, and the <italic>C. elegans </italic>neuronal network, out of a set of network models including a linear preferential attachment model and a small-world model.</p></sec><sec><title>Conclusions</title><p>Our method is a first step towards systematizing network models and assessing their predictability, and we anticipate its usefulness for a number of communities.</p></sec></abstract></article-meta></front><body><sec><title>1 Background</title><p>The post-genomic revolution has ushered in an ensemble of novel crises and opportunities in rethinking molecular biology. The two principal directions in genomics, sequencing and transcriptome studies, have brought to light a number of new questions and forced the development of numerous computational and mathematical tools for their resolution. The sequencing of whole organisms, including <italic>homo sapiens</italic>, has shown that in fact there are roughly the same number of genes, for example, in mice and men. Moreover, much of the coding regions of the chromosomes (the subsequences which are directly translated into proteins) are highly homologous. The complexity comes then, not from a larger number of parts, or more complex parts, but rather through the complexity of their interactions and interconnections.</p><p>Coincident with this biological revolution &#x02013; the massive and unprecedented volume of biological data &#x02013; has blossomed a technological revolution with the popularization and resulting exponential growth of the computing networks. Researchers studying the topology of the Internet [<xref ref-type="bibr" rid="B1">1</xref>] and the World Wide Web [<xref ref-type="bibr" rid="B2">2</xref>] attempted to summarize these topologies via statistical quantities, primarily the distribution <italic>P</italic>(<italic>k</italic>) over nodes of given connectivity or degree <italic>k</italic>, which was found to be completely unlike that of a "random" or Erd&#x000f6;s-R&#x000e9;nyi graph. Instead, the distribution obeyed a power-law <italic>P</italic>(<italic>k</italic>)~<italic>k</italic><sup>-<italic>&#x003b3;</italic></sup>. As a consequence many mathematicians concentrated on (i) measuring the degree distributions of many technological, sociological, and biological graphs (which generically, it turned out, obeyed such power-law distributions) and (ii) proposing various models of randomly-generated graph topologies which could reproduce these degree distributions (<italic>cf</italic>. [<xref ref-type="bibr" rid="B3">3</xref>] for a thorough review). The success of these latter efforts reveals a conundrum for mathematical modeling: a metric which is universal (rather than discriminative) cannot be used for choosing the model which best describes a network of interest. The question posed is one of <italic>classification</italic>, meaning the construction of an algorithm, based on training data from multiple classes, which can place data of interest within one of the classes with small test loss.</p><p>Systematic enumeration of substructures has so far been used to find statistically significant subgraphs or "motifs" [<xref ref-type="bibr" rid="B4">4</xref>-<xref ref-type="bibr" rid="B8">8</xref>] by comparing the network of interest to an assumed null model. Recently, the idea of clustering real networks into groups based on similarity in their "significance profiles" has been proposed [<xref ref-type="bibr" rid="B9">9</xref>]. We here use and extend these ideas to compare a given network of interest to a set of proposed network models. Rather than unsupervised clustering of real networks, we perform supervised classification of network models. In this paper, we present a natural mapping from a graph to an infinite-dimensional vector space using simple operations on the adjacency matrix. The coordinates (called "words", see Methods) reflect the number of various substructures in the network (see Figures <xref ref-type="fig" rid="F3">3</xref> and <xref ref-type="fig" rid="F6">6</xref>). We then use support vector machines (SVMs) to build classifiers that are able to discriminate different network models. The performance of these classifiers is measured using the empirical test-loss on a hold-out set, thus estimating the probability of misclassifying an unseen test network. We selected 17 different mechanisms proposed in the literature to model various properties of naturally occurring networks. Among them are various biologically-inspired graph-generating algorithms which were put forward to model genetic or protein interaction networks. We are then able to classify naturally occurring networks into one of the proposed classes. We here classify data sets for the <italic>E. coli </italic>genetic network, the <italic>C. elegans </italic>neuronal network and the yeast <italic>S. cerevisiae </italic>protein interaction network. To interpret and understand our results further we define a measure of robustness to estimate the confidence of the resulting classification. Moreover, we calculate <italic>p</italic>-values using Gaussian kernel density estimation to find substructures that are characteristic of the network model or the real network of interest. We anticipate that this new approach will provide general tools of network analysis useful to a number of communities.</p></sec><sec><title>Results and Discussion</title><p>We apply our method to three different real data sets: the <italic>E. coli </italic>genetic network [<xref ref-type="bibr" rid="B10">10</xref>] (directed), the <italic>S. cerevisiae </italic>protein interaction network [<xref ref-type="bibr" rid="B11">11</xref>] (undirected), and the <italic>C. elegans </italic>neuronal network [<xref ref-type="bibr" rid="B12">12</xref>] (directed).</p><p>Each node in <italic>E. coli</italic>'s genetic network represents an operon coding for a putative transcriptional factor. An edge exists from operon <italic>i </italic>to operon <italic>j </italic>if operon <italic>i </italic>directly regulates <italic>j </italic>by binding to its operator site. This gives a sparse adjacency matrix with a total of 423 nodes and 519 edges.</p><p>The <italic>S. cerevisiae </italic>protein interaction network has 2114 nodes and 2203 undirected edges. Its sparseness is therefore comparable to that of <italic>E. coli</italic>'s genetic network.</p><p>The <italic>C. elegans </italic>data set represents the organism's fully mapped neuronal network. Here, each node is a neuron and each edge between two nodes represents a functional, directed connection between two neurons. The network consists of 306 neurons and 2359 edges, and is therefore about 7 times more dense than the other two networks. We create training data for undirected or directed models according to the real data set. All parameters other than the numbers of nodes and edges are drawn from a uniform distribution over their range. We sample 1000 examples per model for each real data set, train a pairwise multi-class SVM on 4/5 of the sampled data and test on the 1/5 hold-out set. We determine a prediction by counting votes for the different classes. Table <xref ref-type="table" rid="T1">1</xref> summarizes the main results. All three classifiers show very low test loss and two of them a very high robustness (see Subsection Robustness under Methods). The average number of support vectors is relatively small. Indeed, some pairwise classifiers have as few as three support vectors and more than half of them have zero test loss. All of this suggests the existence of a small subset of words which can distinguish among most of these models.</p><p>The predicted models Kumar [<xref ref-type="bibr" rid="B13">13</xref>], Middendorf-Ziv (MZ) [<xref ref-type="bibr" rid="B14">14</xref>], and Sole [<xref ref-type="bibr" rid="B15">15</xref>] are based on very similar mechanisms of iterated duplication and mutation. The model by Kumar <italic>et al</italic>. was originally meant to explain various properties of the WWW. It is based on a duplication mechanism, where at every iteration a prototype for the newly introduced node is chosen at random, and connected to the prototype's neighbors or other randomly chosen nodes with probability <italic>p</italic>. It is therefore built on an imperfect copying mechanism which can also be interpreted as duplication-mutation, often evoked when considering genetic and protein-interaction networks. Sole is based on a similar idea, but is an undirected model, and allows for two free parameters, a probability controlling the number of edges copied and a probability controlling the number of random edges created. MZ is essentially a directed version of Sole. Moreover, we observe that none of the biological networks were predicted to be generated by preferential attachment even though these networks exhibit power-law degree distributions. The duplication-mutation schemes arise as the most successful. However, it is interesting to note that every duplication-mutation model by construction gives rise to an <italic>effective </italic>preferential attachment [<xref ref-type="bibr" rid="B16">16</xref>]. Our classification results therefore do not dismiss the idea of preferential attachment, but merely the specific model which directly implements this idea.</p><p>Kumar and MZ were classified with almost perfect robustness (see Subsection Robustness under Methods) against 500-dimensional (out of 4680 dimensions) subspace sampling. With 26 different choices of subspaces, <italic>E. coli </italic>was always classified as Kumar. We therefore assess with high confidence that Kumar and MZ come closest to modeling <italic>E. coli </italic>and <italic>C. elegans</italic>, respectively. In the case of Sole and the <italic>S. cerevisiae </italic>protein network we observed fluctuations in the assignment to the best model. 3 out of 22 times <italic>S. cerevisiae </italic>was classified as Vazquez (duplication-mutation), other times as Barabasi (preferential attachment), Klemm (duplication-mutation), Kim (scale-free static), or Flammini (duplication-mutation) depending on the subset of words chosen. This clearly indicates that different features support different models. Therefore the confidence in classifying <italic>S. cerevisiae </italic>to be Sole is limited. The statistical significance of individual words in different models is investigated using kernel density estimation (see Methods) by finding words which maximize <italic>&#x003b7;</italic><sub><italic>ij </italic></sub>&#x02261; <italic>p</italic><sub><italic>i</italic></sub>(<italic>x</italic><sub>0</sub>)/<italic>p</italic><sub><italic>j</italic></sub>(<italic>x</italic><sub>0</sub>) for two different models (<italic>i </italic>and <italic>j</italic>) at a word value of the real data set <italic>x</italic><sub>0</sub>. Figure <xref ref-type="fig" rid="F1">1</xref> shows training data for two different models used to classify the <italic>C. elegans </italic>network: the MZ model [<xref ref-type="bibr" rid="B14">14</xref>] which wins in the classification results, and the runner-up Grindrod model [<xref ref-type="bibr" rid="B17">17</xref>]. The histograms for the word nnz <italic>D</italic>(<italic>AU AD AT AU A</italic>) are shown along with their estimated densities, nnz <italic>D</italic>(<italic>AU AD AT AU A</italic>) extremely disfavors the winning model over its runner-up (minimizes <italic>&#x003b7;</italic><sub><italic>ij</italic></sub>). The opposite case is shown in Figure <xref ref-type="fig" rid="F2">2</xref> for <italic>E. coli</italic>, where the plotted word distribution supports the winning model (Kumar [<xref ref-type="bibr" rid="B13">13</xref>]) and disfavors (maximizes <italic>&#x003b7;</italic><sub><italic>ij</italic></sub>) the runner-up Krapivsky-Bianconi model [<xref ref-type="bibr" rid="B18">18</xref>,<xref ref-type="bibr" rid="B14">14</xref>] (preferential attachment). More specifically we are able to verify that the likelihood to generate a network with <italic>E. coli</italic>'s word values is highest for the Kumar model for most of the words. Indeed, out of 1897 words taking at least 2 integer values for all of the models, the estimated density at the <italic>E. coli </italic>word value was highest for Kumar in 1297 cases, for Krapivsky-Bianconi [<xref ref-type="bibr" rid="B18">18</xref>,<xref ref-type="bibr" rid="B14">14</xref>] in 535 cases and for Krapivsky [<xref ref-type="bibr" rid="B18">18</xref>] in only 65 cases.</p><p>Figure <xref ref-type="fig" rid="F2">2</xref> shows the distributions for the word nnz <italic>D</italic>(<italic>AUT AUT AU AUT A</italic>) which had a maximum ratio of probability density of Kumar over that of Krapivsky-Bianconi at <italic>E. coli</italic>'s word value. In fact, <italic>E. coli </italic>has a zero word count, meaning that none of the associated subgraphs shown in Figure <xref ref-type="fig" rid="F3">3</xref> actually occur in <italic>E. coli</italic>. Four of those subgraphs have a mutual edge which is absent in the <italic>E. coli </italic>network and also impossible to generate in a Kumar graph. Krapivsky-Bianconi graphs allow for mutual edges which could be one of the reasons for a higher count in this word. Another source might be that the fifth subgraph showing a higher order feed-forward loop is more probable to be generated in a Krapivsky-Bianconi graph than in a Kumar graph. This subgraph also has to be absent in the <italic>E. coli </italic>network since it gives a zero word value, demonstrating that both the Kumar and Krapivsky-Bianconi models have a tendency to give rise to a topological structure that does not exist in <italic>E. coli</italic>. This analysis gives an example of how these findings are useful in refining network models and in deepening our understanding of real networks. For further discussions refer to our website. [<xref ref-type="bibr" rid="B14">14</xref>]</p><p>The SVM results suggest that one may only need a small subset of words to separate most of the models. The simplest approach to find such a subset is to look at every word for a given pair of models and compute the best split, then rank words by lowest loss. We find that among the most discriminative words some occur very often, such as, nnz (<italic>AA</italic>) or nnz (<italic>AT A</italic>), which count the pairs of edges attached to the same vertex and either pointing in the same direction or pointing away from each other, respectively. Other frequent words include nnz <italic>D</italic>(<italic>AA</italic>), nnz <italic>D</italic>(<italic>AT A</italic>) and &#x003a3;<italic>U</italic>(<italic>AT A</italic>). Figures <xref ref-type="fig" rid="F4">4</xref> and <xref ref-type="fig" rid="F5">5</xref> show scatter-plots of the training data using the most discriminative three words.</p></sec><sec><title>Conclusions</title><p>We proposed a method to discriminate different network topologies, and we showed how to us the resulting classifier to assess which model out of a set of given network models best describes a network of interest. Moreover, the systematic enumeration of countably infinite features of graphs can be successfully used to find new metrics which are highly efficient in separating various kinds of models. Our method is a first step towards systematizing network models and assessing their predictability, and we anticipate its usefulness for a number of communities.</p></sec><sec sec-type="methods"><title>Methods</title><sec><title>Network models</title><p>We sample training data for undirected graphs from six growth models, one scale-free static model [<xref ref-type="bibr" rid="B19">19</xref>-<xref ref-type="bibr" rid="B21">21</xref>], a small-world model [<xref ref-type="bibr" rid="B22">22</xref>], and the Erd&#x000f6;s-R&#x000e9;nyi model [<xref ref-type="bibr" rid="B23">23</xref>]. Among the six growth models two are based on preferential attachment [<xref ref-type="bibr" rid="B24">24</xref>,<xref ref-type="bibr" rid="B25">25</xref>], three on a duplication-mutation mechanism [<xref ref-type="bibr" rid="B16">16</xref>,<xref ref-type="bibr" rid="B15">15</xref>], and one on purely random growth [<xref ref-type="bibr" rid="B26">26</xref>]. For directed graphs we similarly train on two preferential attachment models [<xref ref-type="bibr" rid="B18">18</xref>], two static models [<xref ref-type="bibr" rid="B17">17</xref>,<xref ref-type="bibr" rid="B27">27</xref>,<xref ref-type="bibr" rid="B20">20</xref>], three duplication-mutation models [<xref ref-type="bibr" rid="B13">13</xref>,<xref ref-type="bibr" rid="B28">28</xref>], and the directed Erd&#x000f6;s-R&#x000e9;nyi model [<xref ref-type="bibr" rid="B23">23</xref>]. More detailed descriptions and source code are available on our website [<xref ref-type="bibr" rid="B14">14</xref>].</p><p>For the (directed) <italic>E. coli </italic>transcriptional network and the (directed) <italic>C. elegans </italic>neuronal network we sample training data for all directed models; for the (undirected) <italic>S. cerevisiae </italic>protein interaction network we sample data for all undirected models. The set of undirected models includes two symmetrized versions of originally directed models [<xref ref-type="bibr" rid="B17">17</xref>,<xref ref-type="bibr" rid="B28">28</xref>]. One should note that properties of a directed model can differ significantly from its symmetrized version. In general, the more network classes allowed, the more completetely word space is explored, and therefore the more specific the classification can be.</p><p>In order to classify real data, we sample training examples of the given models with a fixed total number of nodes <italic>N</italic><sub>0</sub>, and allow a small interval <italic>I</italic><sub><italic>M </italic></sub>of 1&#x02013;2% around the total number of edges <italic>M</italic><sub>0 </sub>of the considered real data set. All additional model parameters are sampled uniformly over a given range (which is specified by the model's authors in most cases, and can otherwise be given reasonable bounds). Such a generated graph is accepted if the number of edges <italic>M </italic>falls within the specified interval <italic>I</italic><sub><italic>M </italic></sub>around <italic>M</italic><sub>0</sub>, thereby creating a distribution of graphs associated with each model which should best describe the real data set with given <italic>N</italic><sub>0 </sub>and <italic>M</italic><sub>0</sub>.</p><p>Some of the models can be described as a generalization of another model. Although a generalized model can overlap with a specific one in its support, word space is sufficiently high-dimensional that such confusing realizations are practically impossible. To build intuition, consider that the Erd&#x000f6;s model itself includes all possible network topologies. Nonetheless there is extremely low test loss with any other models, indicating that it still defines a particular volume in this high-dimensional space. Similarly, very few real networks have non-negligible prediction scores for being classified as Erd&#x000f6;s networks.</p></sec><sec><title>Words</title><p>The input space used for classifying graphs was introduced in our earlier work [<xref ref-type="bibr" rid="B6">6</xref>] as a technique for finding statistically significant features and subgraphs in naturally occurring biological and technological networks. Given the adjacency matrix <italic>A </italic>representing a graph (<italic>i.e</italic>., <italic>A</italic><sub><italic>ij </italic></sub>= 1 iff there exists an edge from <italic>j </italic>to <italic>i</italic>), multiplications of the matrix count the number of walks from one node to another (<italic>i.e</italic>., [<italic>A</italic><sup><italic>n</italic></sup>]<sub><italic>ij </italic></sub>is the number of unique walks from <italic>j </italic>to <italic>i </italic>in <italic>n </italic>steps). Note that the adjacency matrix of an undirected graph is symmetric. The topological structure of a network is characterized by the number of open and closed walks of given length. Those can be found by calculating the diagonal or non-diagonal components of the matrix, respectively. For this we define the projection operation <italic>D </italic>such that</p><p>[<italic>D</italic>(<italic>A</italic>)]<sub><italic>ij </italic></sub>= <italic>A</italic><sub><italic>ij</italic></sub><italic>&#x003b4;</italic><sub><italic>ij </italic></sub>&#x000a0;&#x000a0;&#x000a0; (1)</p><p>and its complement <italic>U </italic>= <italic>I </italic>- <italic>D</italic>. (Note that we do <italic>not </italic>use Einstein's summation convention. Indices <italic>i </italic>and <italic>j </italic>are not summed over.) We define the primitive alphabet {<italic>A</italic>; <italic>T</italic>, <italic>U</italic>, <italic>D</italic>} as the adjacency matrix <italic>A </italic>and the operations <italic>T</italic>, <italic>U</italic>, <italic>D </italic>with the transpose operation <italic>T</italic>(<italic>M</italic>) &#x02261; <italic>M</italic><sup><italic>T</italic></sup>, for any matrix <italic>M </italic>. <italic>T</italic>(<italic>A</italic>) and <italic>A </italic>distinguish walks "up" the graph from walks "down" the graph. From the <italic>letters </italic>of this alphabet we can construct <italic>words </italic>(a series of operations) of arbitrary length. A number of redundancies and trivial cases can be eliminated (for example, the projection operations satisfy <italic>DU </italic>= <italic>UD </italic>= 0) leading to the operational alphabet {<italic>A</italic>, <italic>AT</italic>, <italic>AU</italic>, <italic>AD</italic>, <italic>AUT</italic>}. The resulting word is a matrix representing a set of possible walks, which can be enumerated. An example is shown in Figure <xref ref-type="fig" rid="F6">6</xref>.</p><p>Each word determines two relevant statistics of the network: the number of distinct walks and the number of distinct pairs of endpoints. These two statistics are determined by either summing the entries of the matrix (sum) or counting the number of nonzero elements (nnz) of the matrix, respectively. Thus the two operations sum and nnz map words to integers. This allows us to plot any graph in a high-dimensional data space: the coordinates are the integers resulting from these path-based functionals of the graph's adjacency matrix.</p><p>The coordinates of the infinite-dimensional data space are given by integer-valued functionals</p><p><italic>F</italic>(<italic>L</italic><sub>1</sub><italic>L</italic><sub>2</sub>...<italic>L</italic><sub><italic>n</italic></sub><italic>A</italic>) &#x000a0;&#x000a0;&#x000a0; (2)</p><p>where each <italic>L</italic><sub><italic>i </italic></sub>is a letter of the operational alphabet and <italic>F </italic>is an operator from the set {sum, sum<italic>D</italic>, sum<italic>U</italic>, nnz, nnz <italic>D</italic>, nnz <italic>U</italic>}. We found it necessary only to evaluate words with <italic>n </italic>&#x02264; 4 (counting all walks up to length 5) to construct low test-loss classifiers. Therefore, our word space is a 6 <inline-graphic xlink:href="1471-2105-5-181-i1.gif"/> = 4680-dimensional vector space, but since the words are not linearly independent (<italic>e.g</italic>., sum<italic>U </italic>+ sum<italic>D </italic>= sum), the dimensionality of the manifold explored is actually much smaller. However, we continue to use the full data space since a particular word, though it may be expressed as a linear combination of other words, may be a better discriminator than any of its summands.</p><p>In [<xref ref-type="bibr" rid="B6">6</xref>], we discuss several possible interpretations of words, motivated by algorithms for finding subgraphs. Previously studied metrics can sometimes be interpreted in the context of words. For example, the <italic>transitivity </italic>of a network can be defined as 3 times the number of 3-cycles divided by the number of pairs of edges that are incident on a common vertex. For a loopless graph (without self-interactions), this can also be calculated as a simple expression in word space: sum(<italic>D A A A</italic>)/sum(<italic>U AA</italic>). Note that this expression of transitivity as the quotient of two words implies separation in two dimensions rather than in one. However, there are limitations to word space. For example, a similar measure, the <italic>clustering coefficient</italic>, defined as the average over all vertices of the number of 3-cycles containing the vertex divided by the number of paths of length two centered at that vertex, cannot be easily expressed in word space because vertices must be considered individually to compute this quantity. Of course, the utility of word space is not that it encompasses previously studied metrics, but that it can elucidate new metrics in an unbiased, systematic way.</p></sec><sec><title>SVMs</title><p>A standard classification algorithm which has been used with great success in myriad fields is the <italic>support vector machine</italic>, or SVM [<xref ref-type="bibr" rid="B29">29</xref>]. This technique constructs a hyperplane in a high-dimensional feature space separating two classes from each other. Linear kernels are used for the analysis presented here; extensions to appropriate nonlinear kernels are possible.</p><p>We rely on a freely available C-implementation of SVM-Light [<xref ref-type="bibr" rid="B30">30</xref>], which uses a working set selection method to solve the convex programming problem with Lagrangian</p><p><inline-graphic xlink:href="1471-2105-5-181-i2.gif"/></p><p>with <italic>y</italic><sub><italic>i</italic></sub>(<bold>w</bold>&#x000b7;<bold>x</bold><sub><italic>i </italic></sub>+ <italic>b</italic>) &#x02265; 1 - <italic>&#x003be;</italic><sub><italic>i</italic></sub>; <italic>i </italic>= 1,..., <italic>m </italic>where <italic>f</italic>(<bold>x</bold>) = <bold>w</bold>&#x000b7;<bold>x </bold>+ <italic>b </italic>is the equation of the hyperplane, <bold>x</bold><sub><italic>i </italic></sub>are training examples and <italic>y</italic><sub><italic>i </italic></sub>&#x02208; {-1, +1} their class labels. Here, <italic>C </italic>is a fixed parameter determining the trade-off between small errors <italic>&#x003be;</italic><sub><italic>i </italic></sub>and a large margin 2/|<bold>w</bold>|. We set <italic>C </italic>to a default value <inline-graphic xlink:href="1471-2105-5-181-i3.gif"/>. We observe that training and test losses have a negligible dependence on <italic>C </italic>since most test losses are near or equal to zero even in low-dimensional projections of the data space.</p></sec><sec><title>Robustness</title><p>Our objective is to determine which of a set of proposed models most accurately describes a given real data set. After constructing a classifier enjoying low test loss, we classify our given real data set to find a 'best' model. However, the real network may lie outside of any of the sampled distributions of the proposed models in word space. In this case we interpret our classification as a prediction of the least erroneous model.</p><p>We distinguish between the two cases by noting the following: Consider building a classifier for apples and oranges which is then faced with a grapefruit. The classifier may then decide that, based on the feature size the grapefruit is an apple. However, based on the feature taste the grapefruit is classified as an orange. That is, if we train our classifier on different subsets of words and always get the same prediction, the given real network must come closest to the predicted class based on any given choice of features we might look at. We therefore define a <italic>robust classifier </italic>as one which consistently classifies a test datum in the same class, irrespective of the subset of features chosen. And we measure <italic>robustness </italic>as the ratio of the number of consistent predictions over the total number of subspace-classifications. In this paper we consider robustness for a subspace dimensionality of 500, a significantly small fraction of the total number of dimensions 4680.</p></sec><sec><title>Kernel density estimation</title><p>A generative model, in which one estimates the distribution from which observations are drawn, allows a quantitative measure of model assignment: the probability of observing a given word-value given the model. For a robust classifier, in which assignment is not sensitively dependent on the set of features chosen, the conditional probabilities should consistently be greatest for one class.</p><p>To identify significant features we perform density estimations with Gaussian kernels for each individual word, allowing calculation of <italic>p</italic>(<italic>C </italic>= <italic>c</italic>|<italic>X</italic><sub><italic>j </italic></sub>= <italic>x</italic>), the probability of being assigned to class <italic>c </italic>given a particular value <italic>x </italic>of word <italic>j</italic>. By comparing ratios of likelihood values among the different models, it is therefore possible, for the case of non-robust classifiers, to determine which of the features of a grapefruit come closest to an apple and which features come closest to an orange.</p><p>We compute the estimated density at a word value <italic>x</italic><sub>0 </sub>from the training data <italic>x</italic><sub><italic>i </italic></sub>(<italic>i </italic>= 1,..., <italic>m</italic>) as</p><p><inline-graphic xlink:href="1471-2105-5-181-i4.gif"/></p><p>where we optimize the smoothing parameter <italic>&#x003bb; </italic>by maximizing the average log-probability <italic>Q </italic>of a hold-out set using 5-fold cross-validation. More precisely, we partition the training examples into 5-folds <inline-graphic xlink:href="1471-2105-5-181-i5.gif"/>, where {<italic>f</italic><sub><italic>i</italic></sub>(<italic>j</italic>)}<sub><italic>j </italic></sub>is the set of indices associated with fold <italic>i </italic>(<italic>i </italic>= 1...5). We then maximize</p><p><inline-graphic xlink:href="1471-2105-5-181-i6.gif"/></p><p>as a function of <italic>&#x003bb;</italic>. In all cases we found that <italic>Q</italic>(<italic>&#x003bb;</italic>) had a well pronounced maximum as long as the data was not oversampled. Because words can only take integer values, too many training examples can lead to the situation that the data take exactly the same values with or without the hold-out set. In this case, maximizing <italic>Q</italic>(<italic>&#x003bb;</italic>) corresponds to <italic>p</italic>(<italic>x</italic>, <italic>&#x003bb;</italic>) having single peaks around the integer values, so that <italic>&#x003bb; </italic>tends to zero. Therefore, we restrict the number of training examples to 4<italic>N</italic><sub><italic>v</italic></sub>, where <italic>N</italic><sub><italic>v </italic></sub>is the number of unique integer values taken by the training set. With this restriction <italic>Q</italic>(<italic>&#x003bb;</italic>) showed a well-pronounced maximum at a non-zero <italic>&#x003bb; </italic>for all words and models.</p></sec><sec><title>Word ranking</title><p>The simplest scheme to find new metrics which can distinguish among given models is to take a large number of training examples for a pair of network models and find the optimal split between both classes for every word separately. We then test every one-dimensional classifier on a hold-out set and rank words by lowest test loss.</p></sec><sec><title>Web supplement</title><p>Additional figures, more detailed description of the network models, and detailed results can be found at <ext-link ext-link-type="uri" xlink:href="http://www.columbia.edu/itc/applied/wiggins/netclass"/>.</p></sec><sec><title>Source code</title><p>Source code was written in MATLAB and is downloadable from our our website <ext-link ext-link-type="uri" xlink:href="http://www.columbia.edu/itc/applied/wiggins/netclass"/>.</p></sec></sec><sec><title>Authors' contributions</title><p>MM, EZ, and CW had the original ideas for this paper. CW and LC guided the project. Most of the coding was done by MM and EZ. CA, JH, RK, CL, and GW coded most of the network generation agorithms. The final manuscript was mainly written by MM, EZ, CW, and LC.</p></sec></body><back><ack><sec><title>Acknowledgments</title><p>It is a pleasure to acknowledge useful conversations with C. Leslie, D. Watts, and P. Ginsparg. We also acknowledge the generous support of NSF VIGRE grant DMS-98-10750, NSF ECS-03-32479, and the organizers of the LANL CNLS 2003 meeting and the COSIN midterm meeting 2003.</p></sec></ack><ref-list><ref id="B1"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Faloutsos</surname><given-names>C</given-names></name><name><surname>Faloutsos</surname><given-names>M</given-names></name><name><surname>Faloutsos</surname><given-names>P</given-names></name></person-group><article-title>On power-law relationships of the internet topology</article-title><source>Computer Communications Review</source><year>1999</year><volume>29</volume><fpage>251</fpage><lpage>262</lpage><pub-id pub-id-type="doi">10.1145/316194.316229</pub-id></citation></ref><ref id="B2"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Albert</surname><given-names>R</given-names></name><name><surname>Jeong</surname><given-names>H</given-names></name><name><surname>Barab&#x000e1;si</surname><given-names>AL</given-names></name></person-group><article-title>Diameter of the world-wide web</article-title><source>Nature</source><year>1999</year><volume>401</volume><fpage>130</fpage><lpage>131</lpage><pub-id pub-id-type="doi">10.1038/43601</pub-id></citation></ref><ref id="B3"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Newman</surname><given-names>M</given-names></name></person-group><article-title>The Structure and Function of Complex Networks</article-title><source>SIAM</source><year>2003</year><volume>45</volume><fpage>167</fpage></citation></ref><ref id="B4"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Milo</surname><given-names>R</given-names></name><name><surname>Shen-Orr</surname><given-names>SS</given-names></name><name><surname>Itzkovitz</surname><given-names>S</given-names></name><name><surname>Kashtan</surname><given-names>N</given-names></name><name><surname>Alon</surname><given-names>U</given-names></name></person-group><article-title>Simple building blocks of complex networks</article-title><source>Science</source><year>2002</year><volume>298</volume><fpage>824</fpage><lpage>7</lpage><pub-id pub-id-type="pmid">12399590</pub-id><pub-id pub-id-type="doi">10.1126/science.298.5594.824</pub-id></citation></ref><ref id="B5"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Wuchty</surname><given-names>S</given-names></name><name><surname>Oltvai</surname><given-names>ZN</given-names></name><name><surname>Barab&#x000e1;si</surname><given-names>AL</given-names></name></person-group><article-title>Evolutionary conservation of motif constituents in the yeast protein interaction network</article-title><source>Nat gen</source><year>2003</year><volume>35</volume><fpage>176</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1038/ng1242</pub-id></citation></ref><ref id="B6"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Ziv</surname><given-names>E</given-names></name><name><surname>Koytcheff</surname><given-names>R</given-names></name><name><surname>Wiggins</surname><given-names>CH</given-names></name></person-group><article-title>Novel systematic discovery of statistically significant network features</article-title><source>arXiv:cond-mat/0306610</source><year>2003</year></citation></ref><ref id="B7"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Artzy-Randrup</surname><given-names>Y</given-names></name><name><surname>Fleishman</surname><given-names>SJ</given-names></name><name><surname>Ben-Tal</surname><given-names>N</given-names></name><name><surname>Stone</surname><given-names>L</given-names></name></person-group><article-title>Comment on "Network Motifs: Simple Building Blocks of Complex Networks" and "Superfamilies of Evolved and Designed Networks"</article-title><source>Science</source><year>2004</year><volume>305</volume><fpage>1107</fpage><pub-id pub-id-type="pmid">15326338</pub-id><pub-id pub-id-type="doi">10.1126/science.1099334</pub-id></citation></ref><ref id="B8"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Milo</surname><given-names>R</given-names></name><name><surname>Itzkovitz</surname><given-names>S</given-names></name><name><surname>Kashtan</surname><given-names>N</given-names></name><name><surname>Levitt</surname><given-names>R</given-names></name><name><surname>Alon</surname><given-names>U</given-names></name></person-group><article-title>Response to Comment on "Network Motifs: Simple Building Blocks of Complex Networks" and "Superfamilies of Evolved and Designed Networks"</article-title><source>Science</source><year>2004</year><volume>305</volume><fpage>1107d</fpage><pub-id pub-id-type="doi">10.1126/science.1100519</pub-id></citation></ref><ref id="B9"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Milo</surname><given-names>R</given-names></name><name><surname>Itzkovitz</surname><given-names>S</given-names></name><name><surname>Kashtan</surname><given-names>N</given-names></name><name><surname>Levitt</surname><given-names>R</given-names></name><name><surname>Shen-Orr</surname><given-names>S</given-names></name><name><surname>Ayzenshtat</surname><given-names>I</given-names></name><name><surname>Sheffer</surname><given-names>M</given-names></name><name><surname>Alon</surname><given-names>U</given-names></name></person-group><article-title>Superfamilies of evolved and designed networks</article-title><source>Science</source><year>2004</year><volume>303</volume><fpage>1538</fpage><pub-id pub-id-type="pmid">15001784</pub-id><pub-id pub-id-type="doi">10.1126/science.1089167</pub-id></citation></ref><ref id="B10"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Shen-Orr</surname><given-names>S</given-names></name><name><surname>Milo</surname><given-names>R</given-names></name><name><surname>Mangan</surname><given-names>S</given-names></name><name><surname>Alon</surname><given-names>U</given-names></name></person-group><article-title>Network motifs in the transcriptional regulation network of Escherichia coli</article-title><source>Nature Genetics</source><year>2002</year><volume>31</volume><fpage>64</fpage><lpage>68</lpage><pub-id pub-id-type="pmid">11967538</pub-id><pub-id pub-id-type="doi">10.1038/ng881</pub-id></citation></ref><ref id="B11"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Jeong</surname><given-names>H</given-names></name><name><surname>Mason</surname><given-names>S</given-names></name><name><surname>Barab&#x000e1;si</surname><given-names>A</given-names></name><name><surname>Oltvai</surname><given-names>ZN</given-names></name></person-group><article-title>Lethality and centrality of protein networks</article-title><source>Nature</source><year>2001</year><volume>411</volume><fpage>41</fpage><lpage>42</lpage><pub-id pub-id-type="pmid">11333967</pub-id><pub-id pub-id-type="doi">10.1038/35075138</pub-id></citation></ref><ref id="B12"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>White</surname><given-names>JG</given-names></name><name><surname>Southgate</surname><given-names>E</given-names></name><name><surname>Thompson</surname><given-names>JN</given-names></name><name><surname>Brenner</surname><given-names>S</given-names></name></person-group><article-title>The structure of the nervous system of the nematode C. elegans</article-title><source>Phil Trans of the Royal Society of London</source><year>1986</year><volume>314</volume><fpage>1</fpage><lpage>340</lpage></citation></ref><ref id="B13"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Kumar</surname><given-names>R</given-names></name><name><surname>Raghavan</surname><given-names>P</given-names></name><name><surname>Rajagopalan</surname><given-names>S</given-names></name><name><surname>Sivakumar</surname><given-names>D</given-names></name></person-group><article-title>Stochastic models for the web graph</article-title><source>In Symposium on Foundations of Computer Science FOCS, IEEE</source><year>2000</year></citation></ref><ref id="B14"><citation citation-type="other"><ext-link ext-link-type="uri" xlink:href="http://www.columbia.edu/itc/applied/wiggins/netclass"/></citation></ref><ref id="B15"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Sole</surname><given-names>RV</given-names></name><name><surname>Pastor-Satorras</surname><given-names>R</given-names></name><name><surname>Smith</surname><given-names>E</given-names></name><name><surname>Kepler</surname><given-names>TB</given-names></name></person-group><article-title>A model of large-scale proteome evolution</article-title><source>Advances in Complex Systems</source><year>2002</year><volume>5</volume></citation></ref><ref id="B16"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Vazquez</surname><given-names>A</given-names></name><name><surname>Flammini</surname><given-names>A</given-names></name><name><surname>Maritan</surname><given-names>A</given-names></name><name><surname>Vespignani</surname><given-names>A</given-names></name></person-group><article-title>Modeling of protein interaction networks</article-title><source>ComPlexUs</source><year>2003</year><volume>1</volume></citation></ref><ref id="B17"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Grindrod</surname><given-names>P</given-names></name></person-group><article-title>Range-Dependent Random Graphs and their application to modeling large small-world proteome datasets</article-title><source>Phys Rev E Stat Nonlin Soft Matter Phys</source><year>2002</year><volume>66</volume><fpage>066702</fpage><pub-id pub-id-type="pmid">12513439</pub-id><pub-id pub-id-type="doi">10.1103/PhysRevE.66.066702</pub-id></citation></ref><ref id="B18"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Krapivsky</surname><given-names>PL</given-names></name><name><surname>Rodgers</surname><given-names>GJ</given-names></name><name><surname>Redner</surname><given-names>S</given-names></name></person-group><article-title>Degree distributions of growing networks</article-title><source>Phys Rev Lett</source><year>2001</year><volume>86</volume><fpage>5401</fpage><lpage>5404</lpage><pub-id pub-id-type="pmid">11384508</pub-id><pub-id pub-id-type="doi">10.1103/PhysRevLett.86.5401</pub-id></citation></ref><ref id="B19"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>DH</given-names></name><name><surname>Kahng</surname><given-names>B</given-names></name><name><surname>Kim</surname><given-names>D</given-names></name></person-group><article-title>The q-component static model: modeling social networks</article-title><source>arXiv:cond-mat/0307184</source><year>2003</year></citation></ref><ref id="B20"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Goh</surname><given-names>KI</given-names></name><name><surname>Kahng</surname><given-names>B</given-names></name><name><surname>Kim</surname><given-names>D</given-names></name></person-group><article-title>Universal behavior of load distribution in scale-free networks</article-title><source>Phys Rev Lett</source><year>2001</year><volume>87</volume><fpage>278701</fpage><pub-id pub-id-type="pmid">11800921</pub-id><pub-id pub-id-type="doi">10.1103/PhysRevLett.87.278701</pub-id></citation></ref><ref id="B21"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Caldarelli</surname><given-names>G</given-names></name><name><surname>Capocci</surname><given-names>A</given-names></name><name><surname>Rios</surname><given-names>PDL</given-names></name><name><surname>Munoz</surname><given-names>AM</given-names></name></person-group><article-title>Scale-free networks from varying vertex intrinsic fitness</article-title><source>Phys Rev Lett</source><year>2002</year><volume>89</volume><fpage>258702</fpage><pub-id pub-id-type="pmid">12484927</pub-id><pub-id pub-id-type="doi">10.1103/PhysRevLett.89.258702</pub-id></citation></ref><ref id="B22"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Watts</surname><given-names>D</given-names></name><name><surname>Strogatz</surname><given-names>S</given-names></name></person-group><article-title>Collective dynamics of small-world networks</article-title><source>Nature</source><year>1998</year><volume>393</volume><fpage>202</fpage><lpage>204</lpage><pub-id pub-id-type="pmid">9607745</pub-id><pub-id pub-id-type="doi">10.1038/30918</pub-id></citation></ref><ref id="B23"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Erd&#x000f6;s</surname><given-names>P</given-names></name><name><surname>R&#x000e9;nyi</surname><given-names>A</given-names></name></person-group><article-title>On random graphs</article-title><source>Publicationes Mathematicae</source><year>1959</year><volume>6</volume><fpage>290</fpage><lpage>297</lpage></citation></ref><ref id="B24"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Bianconi</surname><given-names>G</given-names></name><name><surname>Barab&#x000e1;si</surname><given-names>A</given-names></name></person-group><article-title>Competition and multiscaling in evolving networks</article-title><source>Europhys Lett</source><year>2001</year><volume>54</volume><fpage>436</fpage><lpage>442</lpage><pub-id pub-id-type="doi">10.1209/epl/i2001-00260-6</pub-id></citation></ref><ref id="B25"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Barab&#x000e1;si</surname><given-names>A</given-names></name></person-group><article-title>Emergence of scaling in random networks</article-title><source>Science</source><year>1999</year><volume>286</volume><fpage>509</fpage><lpage>512</lpage><pub-id pub-id-type="pmid">10521342</pub-id><pub-id pub-id-type="doi">10.1126/science.286.5439.509</pub-id></citation></ref><ref id="B26"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Callaway</surname><given-names>D</given-names></name><name><surname>Hopcroft</surname><given-names>JE</given-names></name><name><surname>Kleinberg</surname><given-names>JM</given-names></name><name><surname>Newman</surname><given-names>ME</given-names></name><name><surname>Strogatz</surname><given-names>SH</given-names></name></person-group><article-title>Are randomly grown graphs really random?</article-title><source>Phys Rev E Stat Nonlin Soft Matter Phys</source><year>2001</year><volume>64</volume><fpage>041902</fpage><pub-id pub-id-type="pmid">11690047</pub-id><pub-id pub-id-type="doi">10.1103/PhysRevE.64.041902</pub-id></citation></ref><ref id="B27"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Higham</surname><given-names>JD</given-names></name></person-group><article-title>Spectral Reordering of a Range-Dependent Weighted Random Graph</article-title><source>Mathematics Research Report 14, University of Strathclyde</source><year>2003</year></citation></ref><ref id="B28"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Vazquez</surname><given-names>A</given-names></name></person-group><article-title>Knowing a network by walking on it: emergence of scaling</article-title><source>arXiv:cond-mat/0006132</source><year>2002</year></citation></ref><ref id="B29"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Vapnik</surname><given-names>V</given-names></name></person-group><source>The Nature of Statistical Learning Theory</source><year>1995</year><publisher-name>Springer-Verlag, NY, USA</publisher-name></citation></ref><ref id="B30"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Joachims</surname><given-names>T</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Sch&#x000f6;lkopf B, Burges C, Smola A</surname></name></person-group><article-title>Making large-Scale SVM Learning Practical</article-title><source>In Advances in Kernel Methods &#x02013; Support Vector Learning</source><year>1999</year><publisher-name>MIT-Press</publisher-name></citation></ref></ref-list><sec sec-type="display-objects"><title>Figures and Tables</title><fig position="float" id="F1"><label>Figure 1</label><caption><p><bold><italic>C. elegans</italic>: kernel density estimation for the word nnz </bold><italic>D</italic>(<italic>AU AD AT AU A</italic>). Data for two different models are shown: the Middendorf-Ziv [14] model and the Grindrod [17] model. <italic>C. elegans </italic>is robustly classified as a Middendorf-Ziv network. The Grindrod model is the runner-up. We here show data for a word that especially <italic>disfavors </italic>the Middendorf-Ziv model over the Grindrod model. The histograms of the word over the training data are shown along with their associated densities calculated from the data by Gaussian kernel density estimation. The densities give the following log-<italic>p</italic>-values at the word value for the <italic>C. elegans </italic>network: log(<italic>p</italic><sub><italic>MZ</italic></sub>) = -376, log(<italic>p</italic><sub><italic>Grindrod</italic></sub>) = -6.23.</p></caption><graphic xlink:href="1471-2105-5-181-1"/></fig><fig position="float" id="F2"><label>Figure 2</label><caption><p><bold><italic>E. coli</italic>: kernel density estimation for the word nnz </bold><italic>D</italic>(<italic>AUT AUT AU AUT A</italic>). Data for two different models are shown: the Kumar model [13] and the Krapivsky-Bianconi [18, 14] model. <italic>E. coli </italic>is robustly classified as a Kumar network. The Krapivsky-Bianconi model is the runner-up. We here show data for a word that especially <italic>favors </italic>the Kumar model over the Krapivsky-Bianconi model. The histograms of the word over the training data are shown along with their associated densities calculated from the data by Gaussian kernel density estimation. The densities give the following log-<italic>p</italic>-values at the word value for the <italic>E. coli </italic>network: log(<italic>p</italic><sub><italic>Kumar</italic></sub>) = -4.22, log(<italic>p</italic><sub><italic>KB</italic></sub>) = -12.0.</p></caption><graphic xlink:href="1471-2105-5-181-2"/></fig><fig position="float" id="F3"><label>Figure 3</label><caption><p><bold>Subgraphs associated with nnz </bold><italic>D AUT AUT AU AUT A</italic>. Every word can be associated with a set of subgraphs. If the word has a non-zero value for a given network, at least one of these subgraphs must appear. The figure shows the subgraphs associated with the word nnz <italic>D AUT AUT AU AUT A</italic>.</p></caption><graphic xlink:href="1471-2105-5-181-3"/></fig><fig position="float" id="F4"><label>Figure 4</label><caption><p><bold>Distributions of the <italic>E. coli </italic>training data in word space. </bold>The training data for <italic>E. coli </italic>for seven directed models is visualized in a 3-dimensional subspace of word space. The three chosen words were found to be most discriminative according to a word ranking method. Every color is associated with a different model. The point which is occupied by <italic>E. coli </italic>is also indicated. The axis correspond to words which can be associated with sets of subgraphs. If a network has a non-zero word value it must possess at least one of these subgraphs.</p></caption><graphic xlink:href="1471-2105-5-181-4"/></fig><fig position="float" id="F5"><label>Figure 5</label><caption><p><bold>Distributions of the <italic>S. cerevisiae </italic>training data in word space. </bold>The training data for <italic>S. cerevisiae </italic>for seven undirected models is visualized in a 3-dimensional subspace of word space. The three chosen words were found to be most discriminative according to a word ranking method. Every color is associated with a different model. The point which is occupied by <italic>S. cerevisiae </italic>is also indicated. The axis correspond to words which can be associated with sets of subgraphs. If a network has a non-zero word value it must possess at least one of these subgraphs.</p></caption><graphic xlink:href="1471-2105-5-181-5"/></fig><fig position="float" id="F6"><label>Figure 6</label><caption><p><bold>Example for a word and its associated subgraphs. </bold>Every word can be associated with a set of subgraphs. If the word has a non-zero value for a given network, at least one of these subgraphs must appear. The figure shows the subgraphs associated with the word nnz <italic>AT A</italic>. The elements of the matrix <italic>AT A </italic>count these two walks. <italic>T A </italic>corresponds to one step "up" the graph, the following <italic>A </italic>to one step "down". The last node could be either the same as the starting node as in the first subgraph (accounted for by the diagonal part <italic>D AT A</italic>) or a different node as in the second subgraph (accounted for by the non-diagonal part <italic>U AT A</italic>).</p></caption><graphic xlink:href="1471-2105-5-181-6"/></fig><table-wrap position="float" id="T1"><label>Table 1</label><caption><p>Summary of classification results. Results of multi-class SVM: the empirical training loss &#x0003c;<italic>L</italic><sub><italic>tr</italic></sub>&#x0003e; averaged over all pairwise classifiers, the average empirical test loss &#x0003c;<italic>L</italic><sub><italic>tst</italic></sub>&#x0003e;, the average number of support vectors &#x0003c;<italic>N</italic><sub><italic>sv</italic></sub>&#x0003e;, and the winning model (with the highest number of votes from all pairwise classifiers). For the definition of robustness see Methods.</p></caption><table frame="hsides" rules="groups"><thead><tr><td></td><td align="left">E. coli</td><td align="left">C. elegans</td><td align="left">S. cerevisiae</td></tr></thead><tbody><tr><td align="left"><italic>L</italic><sub>
                           <italic>tr</italic>
                        </sub></td><td align="left">1.6%</td><td align="left">0.5%</td><td align="left">2.1%</td></tr><tr><td align="left"><italic>L</italic><sub>
                           <italic>tst</italic>
                        </sub></td><td align="left">1.6%</td><td align="left">0.5%</td><td align="left">1.8%</td></tr><tr><td align="left"><italic>N</italic><sub>
                           <italic>sv</italic>
                        </sub></td><td align="left">109</td><td align="left">51</td><td align="left">106</td></tr><tr><td align="left">Winner</td><td align="left">Kumar [13]</td><td align="left">MZ [14]</td><td align="left">Sole [15]</td></tr><tr><td align="left">Robustness</td><td align="left">1.0</td><td align="left">0.97</td><td align="left">0.64</td></tr></tbody></table></table-wrap></sec></back></article>



