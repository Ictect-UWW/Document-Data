<!DOCTYPE article PUBLIC "-//NLM//DTD Journal Archiving and Interchange DTD v2.3 20070202//EN" "archivearticle.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id><journal-title>BMC Bioinformatics</journal-title><issn pub-type="epub">1471-2105</issn><publisher><publisher-name>BioMed Central</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">15383156</article-id><article-id pub-id-type="pmc">PMC526382</article-id><article-id pub-id-type="publisher-id">1471-2105-5-135</article-id><article-id pub-id-type="doi">10.1186/1471-2105-5-135</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group></article-categories><title-group><article-title>Profiled support vector machines for antisense oligonucleotide efficacy prediction</article-title></title-group><contrib-group><contrib id="A1" corresp="yes" contrib-type="author"><name><surname>Camps-Valls</surname><given-names>Gustavo</given-names></name><xref ref-type="aff" rid="I1">1</xref><email>gustavo.camps@uv.es</email></contrib><contrib id="A2" contrib-type="author"><name><surname>Chalk</surname><given-names>Alistair M</given-names></name><xref ref-type="aff" rid="I2">2</xref><email>Alistair.Chalk@cgb.ki.se</email></contrib><contrib id="A3" contrib-type="author"><name><surname>Serrano-L&#x000f3;pez</surname><given-names>Antonio J</given-names></name><xref ref-type="aff" rid="I1">1</xref><email>antonio.j.serrano@uv.es</email></contrib><contrib id="A4" contrib-type="author"><name><surname>Mart&#x000ed;n-Guerrero</surname><given-names>Jos&#x000e9; D</given-names></name><xref ref-type="aff" rid="I1">1</xref><email>jose.d.martin@uv.es</email></contrib><contrib id="A5" contrib-type="author"><name><surname>Sonnhammer</surname><given-names>Erik LL</given-names></name><xref ref-type="aff" rid="I2">2</xref><email>Erik.Sonnhammer@cgb.ki.se</email></contrib></contrib-group><aff id="I1"><label>1</label>Grup de Processament Digital de Senyals, Universitat de Val&#x000e8;ncia, Spain. C/ Dr. Moliner, 50. 46100 Burjassot, Val&#x000e8;ncia, Spain</aff><aff id="I2"><label>2</label>Center for Genomics and Bioinformatics (CGB), Karolinska Institutet, S-17177, Stockholm, Sweden</aff><pub-date pub-type="collection"><year>2004</year></pub-date><pub-date pub-type="epub"><day>22</day><month>9</month><year>2004</year></pub-date><volume>5</volume><fpage>135</fpage><lpage>135</lpage><ext-link ext-link-type="uri" xlink:href="http://www.biomedcentral.com/1471-2105/5/135"/><history><date date-type="received"><day>7</day><month>5</month><year>2004</year></date><date date-type="accepted"><day>22</day><month>9</month><year>2004</year></date></history><copyright-statement>Copyright &#x000a9; 2004 Camps-Valls et al; licensee BioMed Central Ltd.</copyright-statement><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/2.0"><p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/2.0"/>), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</p></license><abstract><sec><title>Background</title><p>This paper presents the use of Support Vector Machines (SVMs) for prediction and analysis of antisense oligonucleotide (AO) efficacy. The collected database comprises 315 AO molecules including 68 features each, inducing a problem well-suited to SVMs. The task of feature selection is crucial given the presence of noisy or redundant features, and the well-known problem of the <italic>curse of dimensionality</italic>. We propose a two-stage strategy to develop an optimal model: (1) feature selection using correlation analysis, mutual information, and SVM-based recursive feature elimination (SVM-RFE), and (2) AO prediction using standard and profiled SVM formulations. A profiled SVM gives different weights to different parts of the training data to focus the training on the most important regions.</p></sec><sec><title>Results</title><p>In the first stage, the SVM-RFE technique was most efficient and robust in the presence of low number of samples and high input space dimension. This method yielded an optimal subset of 14 representative features, which were all related to energy and sequence motifs. The second stage evaluated the performance of the predictors (overall correlation coefficient between observed and predicted efficacy, <italic>r</italic>; mean error, ME; and root-mean-square-error, RMSE) using 8-fold and minus-one-RNA cross-validation methods. The profiled SVM produced the best results (<italic>r </italic>= 0.44, ME = 0.022, and RMSE= 0.278) and predicted high (&#x0003e;75% inhibition of gene expression) and low efficacy (&#x0003c;25%) AOs with a success rate of 83.3% and 82.9%, respectively, which is better than by previous approaches. A web server for AO prediction is available online at <ext-link ext-link-type="uri" xlink:href="http://aosvm.cgb.ki.se/"/>.</p></sec><sec><title>Conclusions</title><p>The SVM approach is well suited to the AO prediction problem, and yields a prediction accuracy superior to previous methods. The profiled SVM was found to perform better than the standard SVM, suggesting that it could lead to improvements in other prediction problems as well.</p></sec></abstract></article-meta></front><body><sec><title>Background</title><p>The expression of a gene can be inhibited by antisense oligonucleotides (AOs) targeting the mRNA. However, if the target site in the mRNA is picked randomly, typically 20% or less of the AOs are effective inhibitors <italic>in vivo </italic>[<xref ref-type="bibr" rid="B1">1</xref>]. The sequence properties that make an AO effective are not well understood, thus many AOs need to be tested to find good inhibitors, which is time-consuming and costly.</p><p>Antisense oligonucleotides contain 10&#x02013;30 nucleotides complementary to a specific subsequence of an mRNA target, which are designed to bind to targets by standard Watson-Crick base pairing rules. The bound duplex can knockdown gene expression through a number of mechanisms. These are RNase-H mediated cleavage, inteference with translation or splicing and destabilization of the target mRNA [<xref ref-type="bibr" rid="B2">2</xref>-<xref ref-type="bibr" rid="B4">4</xref>]. The AO inhibits gene expression in a specific and reversible manner, a process termed 'Gene knock-down' and all mechanisms leave the AO intact to induce further knock-down. For a comprehensive review of the topic see [<xref ref-type="bibr" rid="B5">5</xref>].</p><p>There are many laboratory-based strategies for selecting AOs. A classical approach is the 'gene-walk' approach, in which 15 or more AOs are evaluated for a gene in order to find a sufficiently effective AO. Methods with higher reliability experimentally determine mRNA regions that are accessible to RNase-H clevage and therefore more likely to be an e'ective site for AOs [<xref ref-type="bibr" rid="B6">6</xref>-<xref ref-type="bibr" rid="B8">8</xref>]. In general, the experimental approaches are time consuming and expensive.</p><p>There are many examples in the literature of experimental groups attempting to correlate AO sequence properties with efficacy. A correlation between binding energy (AO-RNA) and efficacy has been observed [<xref ref-type="bibr" rid="B6">6</xref>,<xref ref-type="bibr" rid="B9">9</xref>]. Particular target secondary structures have been shown to correlate with efficacy [<xref ref-type="bibr" rid="B10">10</xref>]. However, the correlations are not consistently detected across studies. This variation can be due to many factors including biases in the selection of the AOs, varying experimental conditions, or, in cases where computational RNA folding prediction was used, limitations in the structure prediction methods. In [<xref ref-type="bibr" rid="B11">11</xref>], published AOs were examined and recomended values for dimer, hairpin and &#x00394;G to increase the proportion of higher efficacy AOs were given.</p><p>AO selection can be based on either experimental or theoretical approaches (for a review, see [<xref ref-type="bibr" rid="B12">12</xref>]). Computational approaches to AO design have so far focused on prediction of the structure of the target mRNA and from this deriving the accessibility of target regions (e.g. [<xref ref-type="bibr" rid="B12">12</xref>-<xref ref-type="bibr" rid="B19">19</xref>]). Perhaps the most successful method is that of Ding and Lawrence [<xref ref-type="bibr" rid="B19">19</xref>], using a statistical sampling of secondary structures to predict accessible regions to find effective AOs for rabbit <italic>&#x003b2;</italic>-globin. In general, methods have not been evaluated on a broad range of gene targets. Another method is to look for motifs that occur more often in effective AOs. Ten sequence motifs have been identified with a correlation to AO efficacy in [<xref ref-type="bibr" rid="B20">20</xref>], and recently, motifs have been used as the input to neural network models [<xref ref-type="bibr" rid="B21">21</xref>,<xref ref-type="bibr" rid="B22">22</xref>] with reasonable success.</p><p>In this context, the challenge is hence to discover general principles that hold across all AO studies. One approach to discover such principles is to explore a diverse range of sequence properties and incorporate the factors that affect AO efficacy into a computational model for AO design. This requires both a database of tested AOs, such as that produced by [<xref ref-type="bibr" rid="B21">21</xref>,<xref ref-type="bibr" rid="B23">23</xref>], and machine learning methods of model building. The database should be based on large AO screening experiments to ensure comparability. In this context, the use of advanced pattern recognition methods such as neural networks or Support Vector Machines (SVMs) is becoming very popular because of their good capabilities for classification, function approximation and knowledge discovery. In particular, the use of SVMs in bioinformatics has found a natural match because they work efficiently with high input dimension spaces and low number of labeled examples. As a consequence, many biological problems have been solved in this field. The interested reader can visit [<xref ref-type="bibr" rid="B24">24</xref>] for a collection of SVM applications in bioinformatics. However, the use of the SVM has been traditionally attached to the classification problem, and few efforts have been made to tackle the regression (or function approximation) problem.</p><p>This paper proposes the use of SVMs for prediction and analysis of AO efficacy. The collected database comprises 315 AO molecules including 68 features each, which induces <italic>a priori </italic>a well-suited problem to SVMs, given the low number of samples and high input space dimension [<xref ref-type="bibr" rid="B25">25</xref>]. Nevertheless, the problem of feature selection becomes crucial because the number of examples in the database (AO molecules) is low compared to the number of features for each of them and, therefore, overfitting is likely to occur, reducing the performance of the model [<xref ref-type="bibr" rid="B26">26</xref>,<xref ref-type="bibr" rid="B27">27</xref>]. Additionally, being able to explain the obtained solution (in terms of the selected input features) can be as relevant as obtaining the best possible predictor. This is of particular interest in bioinformatics in general and for AO efficacy prediction in particular, as was previously illustrated in [<xref ref-type="bibr" rid="B21">21</xref>,<xref ref-type="bibr" rid="B22">22</xref>]. The issue of feature selection in the SVM framework has received attention in the recent years [<xref ref-type="bibr" rid="B28">28</xref>-<xref ref-type="bibr" rid="B32">32</xref>]. The fact that SVMs are not <italic>drastically </italic>affected by the input space dimensionality has sometimes led to the wrong idea that a feature selection is not necessary at all. The guiding principle of SVMs ensures certain robustness to outliers or abnormal samples in the distribution inherently, but the selection of the optimal subset of features is still an unsolved problem in the literature. We can state that in most applications, the success of machine learning is strongly affected by data quality (redundant, noisy or unreliable information) and thus a feature selection is not only recommendable but mandatory.</p><p>In this paper, we propose a two-stage strategy to tackle the problem:</p><p>1. <italic>Feature selection. </italic>This task is carried out using three techniques: correlation analysis, the mutual information feature selection (MIFS) method, and the SVM-based recursive feature elimination (SVM-RFE).</p><p>2. <italic>AO efficacy prediction. </italic>We develop standard and profiled SVMs to accomplish this task. Several measures of accuracy of the estimations and two cross-validation methods are used in order to attain both significant and robust results.</p></sec><sec sec-type="methods"><title>Methods</title><sec><title>Data collection</title><p>In the present work, we have extended the database used in [<xref ref-type="bibr" rid="B21">21</xref>] by including 68 features for each AO. The so-called AO database (AOdb) was assembled from a selection of AO publications. Published data was incorporated for which: (a) at least 6 AOs were tested under the same experimental conditions, although more than one gene target were allowed; (b) efficacy of the AOs were presented as a percentage of the control level of the target gene expression, either as RNA or protein. No papers were reported matching these criteria before 1990, as is consistent with [<xref ref-type="bibr" rid="B23">23</xref>]. Accompanying this data is the full RNA sequence and accession number (where available) together with positional coordinates of the AOs and the position of the coding sequence. Publication details, cell line used and the chemistry of the AOs were also recorded in the database. The database consists of 315 oligonucleotides from 15 studies testing AO efficacy on 13 genes. The essential information in the database is AO sequence and efficacy expressed as (100% - [% of control expression])/100. For the cases where the same AO is tested in two different laboratories, or twice by the same laboratory the average efficacy is used.</p><p>A set of <italic>a priori </italic>representative parameters was derived from the information contained in the AO sequence collection, including values for: (1) base composition (Number of A/C/G/T, % GC content): (2) RNA-AO binding properties (binding energy, enthalpy, entropy): (3) RNA-AO terminal properties (3' binding energy, 5' binding energy); (4) AO-AO binding properties (Hairpin energy and quality, Dimer energy); and (5) 9 of the 10 verified sequence motifs correlated with efficacy from [<xref ref-type="bibr" rid="B20">20</xref>]. Binding energy calculations were completed using thermodynamic parameters from [<xref ref-type="bibr" rid="B33">33</xref>]. The calculation of dimer energy was made using an ungapped alignment with stacking energies taken from [<xref ref-type="bibr" rid="B34">34</xref>] and a uniform penalty 0.5 for mismatches. Hairpin energy was calculated using both Mfold [<xref ref-type="bibr" rid="B35">35</xref>] and the Vienna package [<xref ref-type="bibr" rid="B36">36</xref>]. Parameters describing cellular uptake and protein interactions were not included, as we have no explicit way of modeling them. A number of additional features were included to complete the AOdb: motifs, AO position, predicted conformation of the target structure, single-strandedness, binding energies from [<xref ref-type="bibr" rid="B14">14</xref>]. For brevity, the complete list and more information on the database can be obtained at [<xref ref-type="bibr" rid="B37">37</xref>]. The database is available under request.</p></sec><sec><title>The feature selection problem</title><p>The Feature Selection Problem (FSP) in a "learning from samples" approach can be defined as choosing a subset of features that achieves the lowest error according to a certain loss functional [<xref ref-type="bibr" rid="B28">28</xref>]. Following a general taxonomy, the FSP can be tackled using <italic>filter </italic>[<xref ref-type="bibr" rid="B38">38</xref>] and <italic>wrapper </italic>[<xref ref-type="bibr" rid="B26">26</xref>] methods. Filter methods use an indirect measure of the quality of the selected features, e.g. evaluating the correlation function between each input feature and the observed output. A faster convergence of the algorithm is thus obtained. On the other hand, wrapper methods use as selection criteria the goodness-of-fit between the inputs and the output provided by the learning machine under consideration, e.g. a neural network. This approach guarantees that, in each step of the algorithm, the selected subset improves performance of the previous one. Filter methods might fail to select the right subset of features if the used criterium deviates from the one used for training the learning machine, whereas wrapper methods can be computationally intensive due to the learning machine has to be retrained for each new set of features. In this paper, we evaluate the performance of SVMs for different subsets of relevant features, which are selected using both filter and wrapper approaches.</p></sec><sec><title>Correlation analysis and mutual information</title><p>A common practice to evaluate the (linear) relationship between each of the <italic>n </italic>input features <inline-graphic xlink:href="1471-2105-5-135-i1.gif"/> and output <inline-graphic xlink:href="1471-2105-5-135-i2.gif"/>, or among pair-wise inputs (<inline-graphic xlink:href="1471-2105-5-135-i3.gif"/> and <inline-graphic xlink:href="1471-2105-5-135-i4.gif"/>) is the use of the correlation function. This is a good method to remove redundant features and to evaluate relationships, but fails when working with low number of samples, or when the assumed linear relationship is not present. When data is considered as the realization of random processes, it is possible to compute the relevance of variables with respect to each other by means of the mutual information (MI) function, which is defined as the difference between entropy of <inline-graphic xlink:href="1471-2105-5-135-i3.gif"/> and the conditional entropy of <inline-graphic xlink:href="1471-2105-5-135-i3.gif"/> given <inline-graphic xlink:href="1471-2105-5-135-i4.gif"/>. The MI function is suitable for assessing the information content of features in tasks where methods like the correlation are prone to mistakes. In fact, the MI function measures a general dependence between features, instead of a linear dependence offered by the correlation function. In [<xref ref-type="bibr" rid="B39">39</xref>], an algorithm called Mutual Information Feature Selection (MIFS) was successfully presented. The method greedily constructs the set of features with high mutual information with the output while trying to minimize the mutual information among chosen features. Thus, the <italic>ith </italic>input feature <inline-graphic xlink:href="1471-2105-5-135-i3.gif"/> included in the set, maximizes <inline-graphic xlink:href="1471-2105-5-135-i5.gif"/> over all remaining features <inline-graphic xlink:href="1471-2105-5-135-i6.gif"/> for some parameter <italic>&#x003b2; </italic>&#x02208; (0,1]. The feature selection procedure is performed iteratively until a desired number of features is reached. We will use the correlation function and the MIFS method as filter methods, i.e. a feature ranking will be provided and only the most important features will be accounted for modeling.</p></sec><sec><title>Support vector regressor (SVR)</title><p>Support Vector Machines are state-of-the-art tools for nonlinear input-output knowledge discovery [<xref ref-type="bibr" rid="B40">40</xref>]. The Support Vector Regressor (SVR) is its implementation for regression and function approximation, which has been used in time series prediction with good results [<xref ref-type="bibr" rid="B41">41</xref>]. Basically, the solution offered by the SVR takes the form <inline-graphic xlink:href="1471-2105-5-135-i7.gif"/>, where <bold>x</bold><sub><italic>i </italic></sub>is an input example, <italic>&#x003c6; </italic>is a nonlinear mapping, <bold>w </bold>is a weight vector and <italic>b </italic>is the bias of the regression function. In the SVR, a fixed desired accuracy <italic>&#x003b5; </italic>is specified a priori and thus one tries to fit a "tube" with radius <italic>&#x003b5; </italic>to the training data. The standard SVR tries to minimize two factors: the norm of the squared weight vector, ||<bold>w</bold>||<sup>2</sup>, and the sum of permitted errors. These two factors are traded-off by using a fixed penalization parameter, <italic>C</italic>. We can formally state the SVR method as follows: given a labeled training data set {(<bold>x</bold><sub><italic>i</italic></sub>, <italic>y</italic><sub><italic>i</italic></sub>), <italic>i </italic>= 1,..., <italic>n</italic>}, where <bold>x</bold><sub><italic>i </italic></sub>&#x02208; &#x0211d;<sup><italic>d </italic></sup>and <italic>y</italic><sub><italic>i </italic></sub>&#x02208; &#x0211d;, and a nonlinear mapping to a higher dimensional space <italic>&#x003c6;</italic>: &#x0211d;<sup><italic>d </italic></sup>&#x02192; &#x0211d;<sup><italic>H </italic></sup>where <italic>d </italic>&#x02264; <italic>H</italic>, find the minimum of the following functional with respect to <bold>w</bold>, <italic>&#x003be;</italic><sub><italic>i</italic></sub>, <inline-graphic xlink:href="1471-2105-5-135-i8.gif"/> and <italic>b</italic>:</p><p><inline-graphic xlink:href="1471-2105-5-135-i9.gif"/></p><p>subject to:</p><p><inline-graphic xlink:href="1471-2105-5-135-i10.gif"/></p><p>where <inline-graphic xlink:href="1471-2105-5-135-i11.gif"/> and <italic>C </italic>are, respectively, positive slack variables to deal training samples with a prediction error larger than <italic>&#x003b5; </italic>(<italic>&#x003b5; </italic>&#x0003e; 0) and the penalization applied to these ones. These two parameters are tuned by the user.</p><p>The usual procedure for solving the SVR introduces the linear restrictions (2)-(4) into (1) by means of Lagrange multipliers <italic>&#x003b1;</italic><sub><italic>i </italic></sub>and <inline-graphic xlink:href="1471-2105-5-135-i12.gif"/> associated to each constraint. The dual functional obtained has to be minimized with respect to primal variables (<bold>w</bold>, <italic>&#x003be;</italic><sub><italic>i </italic></sub>and <inline-graphic xlink:href="1471-2105-5-135-i8.gif"/>) and maximized with respect to dual variables (<italic>&#x003b1;</italic><sub><italic>i</italic></sub>). The optimization of the obtained dual problem is usually solved through quadratic programming procedures [<xref ref-type="bibr" rid="B40">40</xref>,<xref ref-type="bibr" rid="B42">42</xref>], and the final solution provided by the SVR for a test example <bold>x </bold>can be expressed as</p><p><inline-graphic xlink:href="1471-2105-5-135-i13.gif"/></p><p>where only the non-zero Lagrange multipliers account in the solution. The corresponding input examples are called <italic>support vectors </italic>and represent the most critical samples in the distribution.</p><p>An important characteristic of the SVR training methodology is that one does not need to know explicitly the form of the mapping <italic>&#x003c6; </italic>(<bold>x</bold>) but only a kernel function, which maps the samples into a high dimensional space. This kernel function appears in the form of dot products in (5), <italic>K </italic>(<bold>x</bold><sub><italic>i</italic></sub>, <bold>x</bold><sub><italic>j</italic></sub>) = <italic>&#x003c6;</italic>(<bold>x</bold><sub><italic>i</italic></sub>)&#x000b7;<italic>&#x003c6;</italic>(<bold>x</bold><sub><italic>j</italic></sub>) and can be viewed as a measure of similarity between samples. Therefore, in order to train the SVR model, one only has to select a kernel function, its free parameters, the parameter <italic>C</italic>, and the size of the <italic>&#x003b5;</italic>-insensitivity zone. In this paper, we have only used the Gaussian (or Radial Basis Function, RBF) kernel, given by:</p><p><italic>K </italic>(<bold>x</bold><sub><italic>i</italic></sub>, <bold>x</bold><sub><italic>j</italic></sub>) = exp (-<italic>&#x003b3;</italic>||<bold>x</bold><sub><italic>i </italic></sub>- <bold>x</bold><sub><italic>j</italic></sub>||<sup>2</sup>). &#x000a0;&#x000a0;&#x000a0; (6)</p><p>There are some reasons to select the RBF kernel <italic>a priori</italic>. The RBF kernel maps samples into a higher dimensional space so, unlike the linear kernel, it can handle efficiently cases in which the relation between the dependent and independent variables is non-linear. The RBF kernel has less numerical difficulties than sigmoid or linear kernels. In fact, sigmoid kernels behave like RBF for certain parameters [<xref ref-type="bibr" rid="B43">43</xref>,<xref ref-type="bibr" rid="B44">44</xref>] but unfortunately, they are non-positive definite kernels in all situations, which precludes their practical application [<xref ref-type="bibr" rid="B25">25</xref>]. Finally, for using the RBF kernel, only the Gaussian width has to be tuned. For tutorials, publications, and software resources on SVM and kernel-based methods, the reader can visit [<xref ref-type="bibr" rid="B45">45</xref>].</p></sec><sec><title>Recursive feature elimination (SVM-RFE)</title><p>The SVM-RFE algorithm has been recently proposed in [<xref ref-type="bibr" rid="B29">29</xref>] for selecting genes that are relevant for a cancer classification problem. The goal is to find a subset of size <italic>m </italic>among <italic>n </italic>features (<italic>m </italic>&#x0003c;<italic>n</italic>) that maximizes the performance of the predictor for a given measure of accuracy. This is a wrapper method and involves high computational cost. The method is based on a backward sequential selection. One starts with all the features and removes one feature at a time until <italic>m </italic>features are left. Basically, in each iteration, one focuses on the weight vector, which constitutes the solution provided by the SVR and therefore, its analysis is of fundamental relevance to understand the importance of each input feature. The removed feature is the one whose removal minimizes the variation of ||<bold>W</bold>||<sup>2</sup>. Hence, the ranking criterion <italic>R</italic><sub><italic>c </italic></sub>for a given feature <italic>i </italic>is:</p><p><inline-graphic xlink:href="1471-2105-5-135-i14.gif"/></p><p>where <italic>K</italic><sup>(<italic>i</italic>) </sup>is the kernel matrix of training data when feature <italic>i </italic>is removed <inline-graphic xlink:href="1471-2105-5-135-i15.gif"/> and <inline-graphic xlink:href="1471-2105-5-135-i16.gif"/> are the Lagrange multipliers corresponding to sample <italic>k </italic>when the input feature <italic>i </italic>is removed. The idea underlying this procedure is basically to evaluate at each iteration which feature affects less the weight vector norm and, consequently, to remove it.</p></sec></sec><sec><title>Results</title><p>In this section, we present and discuss the results obtained both regarding feature selection and prediction accuracy. Filter and wrapper feature selection methods will provide different subsets of representative features. SVMs are trained for each subset and their performance is evaluated in terms of robustness and accuracy.</p><sec><title>Feature selection</title><p>The first approach to the FSP consisted of performing a correlation analysis in order to identify redundant variables. We adopted a similar strategy followed in [<xref ref-type="bibr" rid="B21">21</xref>], i.e. to remove features correlated to each other at &#x0003e;0.9 (<italic>p </italic>&#x0003c; 0.001), keeping the highest correlation to efficacy. This analysis discarded 12 redundant features out of the 68 original ones, and additionally provided a ranking of the most correlated features to efficacy. We finally selected the 14 top ranked features according to this criterion, ranging in correlation from -0.35 (&#x00394;G) to -0.16 (# Adenine). We selected this number of features for the purpose of a fair comparison with the best subset in [<xref ref-type="bibr" rid="B21">21</xref>]. Table <xref ref-type="table" rid="T1">1</xref> shows selected features in both cases. Note that some di'erences are observed between the present work and [<xref ref-type="bibr" rid="B21">21</xref>] with regards the value of the correlation coefficient, <inline-graphic xlink:href="1471-2105-5-135-i17.gif"/> (first and last columns, respectively). They are due to the facts that (1) we have included here very low efficacy oligos in the calculation, and that (2) because more features were added to the AO database, e.g. predicted secondary structure, oligos had to be discarded when the target RNA was unavailable.</p><p>A feature ranking according to the correlation coefficient can be useful to analyze input-output linear dependencies, but it is not good practice to rely only on this decision to build a model. As a second approach, we ran the MIFS method and selected a desired subset of best 14 features. We selected <italic>&#x003b2; </italic>= 0.75, which yielded a balanced estimation of both the MI with the output (AO efficacy), and the already-selected features. The more important features match the ones selected using the correlation function, but MIFS also included hairpin measurements. This is due to the fact that MIFS is not based on correlatedness but on mutual dependence criteria.</p><p>A third approach was the use of SVMs based on the RFE method. In this task, we trained an SVM to predict AO efficacy using all available features. It should be noted here that RFE is a wrapper method that involves a very high computational burden since the SVM must be retrained in each iteration with the selected features. The best model was selected by evaluating the RMSE (accuracy of the estimations) in the validation set through the 8-fold cross-validation method, which splits the data into eight parts, and uses seven parts for training and the eighth one for validation. The procedure is then repeated eight times. In our implementation, we included the possibility suggested in [<xref ref-type="bibr" rid="B29">29</xref>] by which it is possible to remove chunks of features at each iteration &#x02013;a maximum value around 10 was a suitable option. In our application, only ten iterations were necessary to achieve the best 14 features (see Table <xref ref-type="table" rid="T1">1</xref>). In [<xref ref-type="bibr" rid="B20">20</xref>,<xref ref-type="bibr" rid="B21">21</xref>], a surprising lack of correlation was observed between dimer energy and efficacy, which was attributed to some kind of bias in the databases. In the present work, nevertheless, SVM-RFE includes dimer energy as the 11th most relevant feature. In conclusion, SVM-RFE selects a combination of highly correlated but also mutually informative features.</p><p>We can also conclude that noticeable differences are observed between the obtained rankings. A possible explanation for discrepancies of this sort is the non-linear mapping that SVR methods perform. Explaining those input-output relationships is often difficult and biased conclusions are usually obtained. Different families of methods (SVM, neurofuzzy, decision trees, or neural networks) perform different mappings due to their specific guiding principles (structural risk minimization, membership optimization, entropy-based criteria, or empirical risk minimization, respectively) and thus, the interpretation of these methods is quite diffcult. In addition, different models (topologies, structures, kernels, membership functions) in a family would surely yield different results.</p></sec><sec><title>Model development</title><p>A greedy search was carried out for the free parameters (<italic>C</italic>, <italic>&#x003b5;</italic>, <italic>&#x003b3;</italic>) As regards the penalization parameter, it is a common practice trying exponentially increase sequences of <italic>C </italic>(<italic>C </italic>= 10<sup>-2</sup>, 10<sup>-1</sup>,..., 10<sup>3</sup>). In our case study, we achieved good results in the range of <italic>C </italic>&#x02208; [1,1000]. The insensitivity zone was varied linearly in the range [0.001, 0.3]. The <italic>&#x003b3; </italic>parameter was exponentially varied in the range <italic>&#x003b3; </italic>= 10<sup>-7</sup>,...,10<sup>-1</sup>. For each free parameter combination, we evaluated the performance of the predictors through several measurements: the correlation coefficient between actual and predicted efficacies (<italic>r</italic>), the mean error (ME), and the root-mean-square-error (RMSE). Additionally, we computed the rate of observed efficacies above a defined predicted threshold of 0.75 (SR<sub>&#x0003e;0.75</sub>) and below 0.25 (SR<sub>&#x0003c;0.25</sub>). These prediction ranges are of particular interest, since they stand for high and low AO efficacies, respectively. In fact, it is not only important to identify high efficacy oligos but also factors causing AOs to be completely ineffective ([0,0.25]). However, care must be taken as more noise can be present in the low efficacy region.</p></sec><sec><title>Model comparison</title><p>At the first stage of the work, we trained SVMs using the 8-fold cross-validation method for RFE-based feature selection. However, this training methodology can lead to overoptimistic results because AOs on the same gene are not always independent data points. Hence, we also followed a different strategy, which entails removing all AOs targeting one gene for training, training the model, and then testing performance on predicting the efficacy of these oligos. This is a common method [<xref ref-type="bibr" rid="B22">22</xref>] and we refer to it as minus-one-RNA cross-validation (-RNA). It safely removes any overlap between training and test data, and thus ensures the generality of the model.</p><p>In AO prediction, we are most interested in predicting good oligos (high efficacy, &#x0003e; 0.75), and those that are bad (low efficacy, &#x0003c; 0.25). This previous knowledge about the problem can be introduced in the SVM formulation by tailoring specific <italic>confidence functions </italic>for the adaptation of the penalization factor <italic>C</italic>, and the <italic>&#x003b5;</italic>-insensitive zone of each sample. The so-called Profiled SVR (P-SVR) [<xref ref-type="bibr" rid="B46">46</xref>] obviously implies making some changes in the original SVR formulation since now <italic>C </italic>and <italic>&#x003b5; </italic>become sample-dependent. In [<xref ref-type="bibr" rid="B46">46</xref>,<xref ref-type="bibr" rid="B47">47</xref>], we designed profiles for the variation of <italic>C </italic>and <italic>&#x003b5; </italic>in complex pharmacokinetic problems. In this paper, our intention relaxing or tightening <italic>&#x003b5; </italic>and <italic>C </italic>depending on the observed AO efficacy value. A proposal for this variation is illustrated in figure <xref ref-type="fig" rid="F1">1</xref>. Note that we increase the penalization of errors committed in the high or low AO efficacy ranges since we are more interested in obtaining good results in these regions.</p><p>Additionally, the <italic>&#x003b5;</italic>-insensitivity zone is reduced in these regions thus forcing a reduced error there. Some other profiles could be introduced in the training methodology without loss of generality.</p><p>Results for all approaches are shown in Table <xref ref-type="table" rid="T2">2</xref> for the validation set. We observe that RFE is the best method for selecting features. The choice of cross-validation method does not make much difference; the RMSE is the same while the goodness-of-fit (<italic>r</italic>) is almost unchanged. Using the P-SVR method (with features selected by the 8-fold crossvalidated RFE) we gained substantially in RMSE, and also obtained a better balance between the success rates of high and low predictions. This indicates that the P-SVR improves the performance of standard SVR even without a dedicated feature selection method, and suggests that even better results could be obtained if P-SVR were embedded in the RFE feature selection procedure.</p><p>These outcomes are worth analyzing because one could expect worse results when using -RNA cross-validation since this method removes the possibility of cross-talk in the training phase between overlapping oligos. However, we have to stress here that, by training the SVR with -RNA cross-validation, one only improves the <italic>r </italic>indicator, which is a biased estimator of the accuracy. In fact, accuracy (RMSE) remains basically the same, and bias (ME) becomes positive and higher, which could induce some distrust for the model. When analyzing results from the P-SVR, we can observe a general improvement in all indicators, which is basically due to the fact that by tightening the "tube" around the interesting ranges, a higher number of support vectors is selected there (but lower in the overall domain), which induces a richer solution in the interesting zones. In addition, the profiled <italic>C </italic>parameter penalizes higher the committed errors in these zones, which is particularly interesting to deal with outlying samples in the distribution and to provide a smoother solution in these particular zones. The designed profile, nevertheless, could lead to an overfitted solution in the interesting zone if <italic>&#x003b5;</italic><sub><italic>i </italic></sub>and <italic>C</italic><sub><italic>i </italic></sub>were not well-controlled. However, by using the-RNA cross-validation method, this threat is avoided and better results are finally obtained. Therefore, the combined strategy of P-SVR and -RNA cross-validation results in a balanced and robust predictor. Additional consequences can be extracted: (1) the correlation coefficient is relatively low for all methods but superior to the ones obtained in [<xref ref-type="bibr" rid="B21">21</xref>]; (2) differences among the models are neither numerically (see Table <xref ref-type="table" rid="T2">2</xref>) nor statistically significant as tested with One Way Analysis-Of-Variance (ANOVA) in bias (<italic>F </italic>= 0.01, <italic>p </italic>= 0.811) or accuracy (<italic>F </italic>= 0.06, <italic>p </italic>= 0.567); (3) prediction is more accurate, in general terms, for the higher efficacy levels (SR<sub>&#x0003e;0.75</sub>, &#x0003e; SR<sub>&#x0003c;0.25</sub>), as also noted in [<xref ref-type="bibr" rid="B22">22</xref>]; and (4) SVM-RFE can deal efficiently with high input spaces and produces robust results (compare results with those from the "All features" subset). Additionally, we can conclude that the P-SVR improved results in terms of accuracy of the predictions compared to the standard SVR.</p></sec></sec><sec><title>Conclusions</title><p>In this paper, we have used standard and state-of-the-art methods for knowledge discovery in a relevant bioinformatics problem: the analysis and prediction of AO efficacy. We have engineered robust and accurate SVMs, and used filter and wrapper feature selection methods in order to build representative subsets of input features. Compared to [<xref ref-type="bibr" rid="B21">21</xref>], our results represent a significant improvement. In that work, SR<sub>&#x0003e;0.8 </sub>was reported to be 50%, and <italic>r </italic>= 0.30. The success of the P-SVR for the AO prediction problem suggests that it could be successfully applied to other prediction problems. A web server for AO prediction is available online at [<xref ref-type="bibr" rid="B48">48</xref>].</p><p>Our future work is concentrated to improving results with more careful design of profiles by the inclusion of fuzzy and rough sets. Additionally, we are exploring the possibility of providing confidence values for the predictions in the form of <italic>p</italic>-values from the Lagrange multipliers. This way, the user could get a set of best predictions back, then a second set that is more likely to be less accurate, and so on. This would allow the lab-user to choose the best ones first, but if they fail specificity controls they would have another set to work with.</p></sec><sec><title>Authors' contributions</title><p>GCV carried out the training of the feature selection and regression methods. AC participated in model development and testing process, and developed the web-server. AJSL collaborated in model development and assessment. JDMG engineered the profile function. ES conceived and coordinated the study. All authors contributed to the manuscript preparation, and approved the final manuscript.</p></sec></body><back><ref-list><ref id="B1"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Myers</surname><given-names>K</given-names></name><name><surname>Dean</surname><given-names>N</given-names></name></person-group><article-title>Sensible use of antisense: how to use oligonucleotides as research tools</article-title><source>Trends Pharmacol Sci</source><year>2000</year><volume>21</volume><fpage>19</fpage><lpage>23</lpage><pub-id pub-id-type="pmid">10637651</pub-id><pub-id pub-id-type="doi">10.1016/S0165-6147(99)01420-0</pub-id></citation></ref><ref id="B2"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Wahlestedt</surname><given-names>C</given-names></name></person-group><article-title>Antisense oligonucleotide strategies in neuropharmacology</article-title><source>Trends Pharmacol Sci</source><year>1994</year><volume>15</volume><fpage>42</fpage><lpage>46</lpage><pub-id pub-id-type="pmid">8165722</pub-id><pub-id pub-id-type="doi">10.1016/0165-6147(94)90107-4</pub-id></citation></ref><ref id="B3"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Agrawal</surname><given-names>S</given-names></name><name><surname>Zhao</surname><given-names>Q</given-names></name></person-group><article-title>Antisense therapeutics in neuropharmacology</article-title><source>Curr Opin Chem Biol</source><year>1998</year><volume>2</volume><fpage>519</fpage><lpage>528</lpage><pub-id pub-id-type="pmid">9736926</pub-id><pub-id pub-id-type="doi">10.1016/S1367-5931(98)80129-4</pub-id></citation></ref><ref id="B4"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Bennett</surname><given-names>C</given-names></name><name><surname>Cowsert</surname><given-names>L</given-names></name></person-group><article-title>Application of antisense oligonucleotide for gene functionalization and target validation</article-title><source>Curr Opin Mol Ther</source><year>1999</year><volume>1</volume><fpage>359</fpage><lpage>371</lpage><pub-id pub-id-type="pmid">11713801</pub-id></citation></ref><ref id="B5"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Crooke</surname><given-names>S</given-names></name></person-group><article-title>Progress in antisense technology: the end of the beginning</article-title><source>Methods Enzymol</source><year>2000</year><volume>313</volume><fpage>3</fpage><lpage>45</lpage><pub-id pub-id-type="pmid">10595347</pub-id><pub-id pub-id-type="doi">10.1016/S0076-6879(00)13003-4</pub-id></citation></ref><ref id="B6"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Ho</surname><given-names>S</given-names></name><name><surname>Bao</surname><given-names>Y</given-names></name><name><surname>Lesher</surname><given-names>T</given-names></name><name><surname>Malhotra</surname><given-names>R</given-names></name><name><surname>Ma</surname><given-names>L</given-names></name><name><surname>Fluharty</surname><given-names>S</given-names></name><name><surname>Sakai</surname><given-names>R</given-names></name></person-group><article-title>Mapping of RNA accessible sites for anti-sense experiments with oligonucleotide libraries</article-title><source>Nat Biotechnol</source><year>1998</year><volume>16</volume><fpage>59</fpage><lpage>63</lpage><pub-id pub-id-type="pmid">9447595</pub-id><pub-id pub-id-type="doi">10.1038/nbt0198-59</pub-id></citation></ref><ref id="B7"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Milner</surname><given-names>N</given-names></name><name><surname>Mir</surname><given-names>K</given-names></name><name><surname>Southern</surname><given-names>E</given-names></name></person-group><article-title>Selecting effective antisense reagents on combinatorial oligonucleotide arrays</article-title><source>Nat Biotechnol</source><year>1997</year><volume>15</volume><fpage>537</fpage><lpage>541</lpage><pub-id pub-id-type="pmid">9181575</pub-id><pub-id pub-id-type="doi">10.1038/nbt0697-537</pub-id></citation></ref><ref id="B8"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>H</given-names></name><name><surname>Mao</surname><given-names>J</given-names></name><name><surname>Zhou</surname><given-names>D</given-names></name><name><surname>Xu</surname><given-names>Y</given-names></name><name><surname>Thonberg</surname><given-names>H</given-names></name><name><surname>Liang</surname><given-names>Z</given-names></name><name><surname>Wahlestedt</surname><given-names>C</given-names></name></person-group><article-title>mRNA accessible site tagging (MAST): a novel high throughput method for selecting effective antisense oligonucleotides</article-title><source>Nucleic Acids Res</source><year>2003</year><volume>31</volume><fpage>e72</fpage><pub-id pub-id-type="pmid">12853649</pub-id><pub-id pub-id-type="doi">10.1093/nar/gng072</pub-id></citation></ref><ref id="B9"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Ho</surname><given-names>S</given-names></name><name><surname>Britton</surname><given-names>D</given-names></name><name><surname>Stone</surname><given-names>B</given-names></name><name><surname>Behrens</surname><given-names>D</given-names></name><name><surname>Leffet</surname><given-names>L</given-names></name><name><surname>Hobbs</surname><given-names>F</given-names></name><name><surname>Miller</surname><given-names>J</given-names></name><name><surname>Trainor</surname><given-names>G</given-names></name></person-group><article-title>Potent antisense oligonucleotides to the human multidrug resistance-1 mRNA are rationally selected by mapping RNA-accessible sites with oligonucleotide libraries</article-title><source>Nucleic Acids Res</source><year>1996</year><volume>24</volume><fpage>1901</fpage><lpage>1907</lpage><pub-id pub-id-type="pmid">8657572</pub-id><pub-id pub-id-type="doi">10.1093/nar/24.10.1901</pub-id></citation></ref><ref id="B10"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Vickers</surname><given-names>T</given-names></name><name><surname>Wyatt</surname><given-names>J</given-names></name><name><surname>Freier</surname><given-names>S</given-names></name></person-group><article-title>Effects of RNA secondary structure on cellular antisense activity</article-title><source>Nucleic Acids Res</source><year>2000</year><volume>28</volume><fpage>1340</fpage><lpage>1347</lpage><pub-id pub-id-type="pmid">10684928</pub-id><pub-id pub-id-type="doi">10.1093/nar/28.6.1340</pub-id></citation></ref><ref id="B11"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Matveeva</surname><given-names>O</given-names></name><name><surname>Mathews</surname><given-names>D</given-names></name><name><surname>Tsodikov</surname><given-names>A</given-names></name><name><surname>Shabalina</surname><given-names>S</given-names></name><name><surname>Gesteland</surname><given-names>R</given-names></name><name><surname>Atkins</surname><given-names>J</given-names></name><name><surname>Freier</surname><given-names>S</given-names></name></person-group><article-title>Thermodynamic criteria for high hit rate antisense oligonucleotide design</article-title><source>Nucleic Acids Res</source><year>2003</year><volume>31</volume><fpage>4989</fpage><lpage>4994</lpage><pub-id pub-id-type="pmid">12930948</pub-id><pub-id pub-id-type="doi">10.1093/nar/gkg710</pub-id></citation></ref><ref id="B12"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Sczakiel</surname><given-names>G</given-names></name></person-group><article-title>Theoretical and experimental approaches to design effective antisense oligonucleotides</article-title><source>Front Biosci</source><year>2000</year><volume>5</volume><fpage>D194</fpage><lpage>201</lpage><pub-id pub-id-type="pmid">10702382</pub-id></citation></ref><ref id="B13"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Mathews</surname><given-names>D</given-names></name><name><surname>Sabina</surname><given-names>J</given-names></name><name><surname>Zuker</surname><given-names>M</given-names></name><name><surname>Turner</surname><given-names>D</given-names></name></person-group><article-title>Expanded sequence dependence of thermodynamic parameters improves prediction of RNA secondary structure</article-title><source>J Mol Biol</source><year>1999</year><volume>288</volume><fpage>911</fpage><lpage>940</lpage><pub-id pub-id-type="pmid">10329189</pub-id><pub-id pub-id-type="doi">10.1006/jmbi.1999.2700</pub-id></citation></ref><ref id="B14"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Walton</surname><given-names>S</given-names></name><name><surname>Stephanopoulos</surname><given-names>G</given-names></name><name><surname>Yarmush</surname><given-names>M</given-names></name><name><surname>Roth</surname><given-names>C</given-names></name></person-group><article-title>Prediction of antisense oligonucleotide binding anity to a structured RNA target</article-title><source>Biotechnol Bioeng</source><year>1999</year><volume>65</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="pmid">10440665</pub-id><pub-id pub-id-type="doi">10.1002/(SICI)1097-0290(19991005)65:1&#x0003c;1::AID-BIT1&#x0003e;3.3.CO;2-6</pub-id></citation></ref><ref id="B15"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Amarzguioui</surname><given-names>M</given-names></name><name><surname>Brede</surname><given-names>G</given-names></name><name><surname>Babaie</surname><given-names>E</given-names></name><name><surname>Grotli</surname><given-names>M</given-names></name><name><surname>Sproat</surname><given-names>B</given-names></name><name><surname>Prydz</surname><given-names>H</given-names></name></person-group><article-title>Secondary structure prediction and in vitro accessibility of mRNA as tools in the selection of target sites for ribozymes</article-title><source>Nucleic Acids Res</source><year>2000</year><volume>28</volume><fpage>4113</fpage><lpage>4124</lpage><pub-id pub-id-type="pmid">11058107</pub-id><pub-id pub-id-type="doi">10.1093/nar/28.21.4113</pub-id></citation></ref><ref id="B16"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Song</surname><given-names>H</given-names></name><name><surname>Tang</surname><given-names>Z</given-names></name><name><surname>Yuan</surname><given-names>S</given-names></name><name><surname>Zhu</surname><given-names>B</given-names></name></person-group><article-title>Application of secondary structure prediction in antisense drug design targeting protein kinase C-alpha mRNA and QSAR analysis</article-title><source>Acta Pharmacol Sin</source><year>2000</year><volume>21</volume><fpage>80</fpage><lpage>86</lpage><pub-id pub-id-type="pmid">11263253</pub-id></citation></ref><ref id="B17"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Scherr</surname><given-names>M</given-names></name><name><surname>Rossi</surname><given-names>J</given-names></name><name><surname>Sczakiel</surname><given-names>G</given-names></name><name><surname>Patzel</surname><given-names>V</given-names></name></person-group><article-title>RNA accessibility prediction: a theoretical approach is consistent with experimental studies in cell extracts</article-title><source>Nucleic Acids Res</source><year>2000</year><volume>28</volume><fpage>2455</fpage><lpage>2461</lpage><pub-id pub-id-type="pmid">10871393</pub-id><pub-id pub-id-type="doi">10.1093/nar/28.13.2455</pub-id></citation></ref><ref id="B18"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Toschi</surname><given-names>N</given-names></name></person-group><article-title>Influence of mRNA self-structure on hybridization: computational tools for antisense sequence selection</article-title><source>Methods</source><year>2000</year><volume>22</volume><fpage>261</fpage><lpage>269</lpage><pub-id pub-id-type="pmid">11071822</pub-id><pub-id pub-id-type="doi">10.1006/meth.2000.1078</pub-id></citation></ref><ref id="B19"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname><given-names>Y</given-names></name><name><surname>Lawrence</surname><given-names>C</given-names></name></person-group><article-title>Statistical prediction of single-stranded regions in RNA secondary structure and application to predicting effective antisense target sites and beyond</article-title><source>Nucleic Acids Res</source><year>2001</year><volume>29</volume><fpage>1034</fpage><lpage>1046</lpage><pub-id pub-id-type="pmid">11222752</pub-id><pub-id pub-id-type="doi">10.1093/nar/29.5.1034</pub-id></citation></ref><ref id="B20"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Matveeva</surname><given-names>O</given-names></name><name><surname>Tsodikov</surname><given-names>A</given-names></name><name><surname>Giddings</surname><given-names>M</given-names></name><name><surname>Freier</surname><given-names>S</given-names></name><name><surname>Wyatt</surname><given-names>J</given-names></name><name><surname>Spiridonov</surname><given-names>A</given-names></name><name><surname>Shabalina</surname><given-names>S</given-names></name><name><surname>Gesteland</surname><given-names>R</given-names></name><name><surname>Atkins</surname><given-names>J</given-names></name></person-group><article-title>Identification of sequence motifs in oligonucleotides whose presence is correlated with antisense activity</article-title><source>Nucleic Acids Res</source><year>2000</year><volume>28</volume><fpage>2862</fpage><lpage>2865</lpage><pub-id pub-id-type="pmid">10908347</pub-id><pub-id pub-id-type="doi">10.1093/nar/28.15.2862</pub-id></citation></ref><ref id="B21"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Chalk</surname><given-names>A</given-names></name><name><surname>Sonnhammer</surname><given-names>E</given-names></name></person-group><article-title>Computational antisense oligo prediction with a neural network model</article-title><source>Bioinformatics</source><year>2002</year><volume>18</volume><fpage>1567</fpage><lpage>1575</lpage><pub-id pub-id-type="pmid">12490440</pub-id><pub-id pub-id-type="doi">10.1093/bioinformatics/18.12.1567</pub-id></citation></ref><ref id="B22"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Giddings</surname><given-names>MC</given-names></name><name><surname>Shah</surname><given-names>AA</given-names></name><name><surname>Freier</surname><given-names>S</given-names></name><name><surname>Atkins</surname><given-names>JF</given-names></name><name><surname>Gesteland</surname><given-names>RF</given-names></name><name><surname>Matveeva</surname><given-names>OV</given-names></name></person-group><article-title>Artificial neural network prediction of antisense oligodeoxynucleotide activity</article-title><source>Nucleic Acids Research</source><year>2002</year><volume>30</volume><fpage>4295</fpage><lpage>4304</lpage><pub-id pub-id-type="pmid">12364609</pub-id><pub-id pub-id-type="doi">10.1093/nar/gkf557</pub-id></citation></ref><ref id="B23"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Giddings</surname><given-names>M</given-names></name><name><surname>Matveeva</surname><given-names>O</given-names></name><name><surname>Atkins</surname><given-names>J</given-names></name><name><surname>Gesteland</surname><given-names>R</given-names></name></person-group><article-title>ODNBase &#x02013; A web database for antisense oligonucleotide effectiveness studies</article-title><source>Bioinformatics</source><year>2000</year><volume>16</volume><fpage>843</fpage><lpage>844</lpage><pub-id pub-id-type="pmid">11108708</pub-id><pub-id pub-id-type="doi">10.1093/bioinformatics/16.9.843</pub-id></citation></ref><ref id="B24"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Cristianini</surname><given-names>N</given-names></name><name><surname>Shawe-Taylor</surname><given-names>J</given-names></name></person-group><article-title>Support Vector Machines in Bioinformatics</article-title><year>2004</year><ext-link ext-link-type="uri" xlink:href="http://www.support-vector.net/bioinformatics.html"/><comment>Last visited September 1st 2004</comment></citation></ref><ref id="B25"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Sch&#x000f6;lkopf</surname><given-names>B</given-names></name><name><surname>Smola</surname><given-names>A</given-names></name></person-group><source>Learning with Kernels &#x02013; Support Vector Machines, Regularization, Optimization and Beyond</source><year>2001</year><publisher-name>MIT Press Series</publisher-name></citation></ref><ref id="B26"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Kohavi</surname><given-names>R</given-names></name><name><surname>John</surname><given-names>GH</given-names></name></person-group><article-title>Wrappers for features subset selection</article-title><source>Int J Digit Libr</source><year>1997</year><volume>1</volume><fpage>108</fpage><lpage>121</lpage><pub-id pub-id-type="doi">10.1007/s007990050008</pub-id></citation></ref><ref id="B27"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>J</given-names></name><name><surname>Honavar</surname><given-names>V</given-names></name></person-group><article-title>Feature subset selection using a genetic algorithm</article-title><source>IEEE Intelligent Systems</source><year>1998</year><volume>13</volume><fpage>44</fpage><lpage>49</lpage><pub-id pub-id-type="doi">10.1109/5254.671091</pub-id></citation></ref><ref id="B28"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Weston</surname><given-names>H</given-names></name><name><surname>Mukherjee</surname><given-names>S</given-names></name><name><surname>Chapelle</surname><given-names>O</given-names></name><name><surname>Pontil</surname><given-names>M</given-names></name><name><surname>Poggio</surname><given-names>T</given-names></name><name><surname>Vapnik</surname><given-names>V</given-names></name></person-group><article-title>Feature selection for SVMs</article-title><source>Advances in Neural Information Processing Systems, NIPS</source><year>2000</year><volume>12</volume><publisher-name>MIT Press</publisher-name><fpage>526</fpage><lpage>532</lpage></citation></ref><ref id="B29"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Guyon</surname><given-names>I</given-names></name><name><surname>Weston</surname><given-names>J</given-names></name><name><surname>Barnhill</surname><given-names>S</given-names></name><name><surname>Vapnik</surname><given-names>V</given-names></name></person-group><article-title>Gene Selection for Cancer Classification using Support Vector Machines</article-title><source>Machine Learning</source><year>2002</year><volume>46</volume><fpage>389</fpage><lpage>422</lpage><pub-id pub-id-type="doi">10.1023/A:1012487302797</pub-id></citation></ref><ref id="B30"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Weston</surname><given-names>J</given-names></name><name><surname>P&#x000e9;rez-Cruz</surname><given-names>F</given-names></name><name><surname>Bousquet</surname><given-names>O</given-names></name><name><surname>Chapelle</surname><given-names>O</given-names></name><name><surname>Elisseeff</surname><given-names>A</given-names></name><name><surname>Bernhard</surname><given-names>S</given-names></name></person-group><article-title>Feature selection and transduction for prediction of molecular bioactivity for drug design</article-title><source>Bioinformatics</source><year>2003</year><volume>19</volume><fpage>764</fpage><lpage>771</lpage><pub-id pub-id-type="pmid">12691989</pub-id><pub-id pub-id-type="doi">10.1093/bioinformatics/btg054</pub-id></citation></ref><ref id="B31"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Rakotomamonjy</surname><given-names>A</given-names></name></person-group><article-title>Variable Selection Using SVM-based Criteria</article-title><source>Journal of Machine Learning Research (JMLR)</source><year>2003</year><volume>3</volume><fpage>1357</fpage><lpage>1370</lpage><pub-id pub-id-type="doi">10.1162/153244303322753706</pub-id></citation></ref><ref id="B32"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Byvatov</surname><given-names>E</given-names></name><name><surname>Schneider</surname><given-names>G</given-names></name></person-group><article-title>SVM-Based Feature Selection for Characterization of Focused Compound Collections</article-title><source>J Chem Inf Comput Sci</source><year>2004</year><volume>44</volume><fpage>993</fpage><lpage>999</lpage><pub-id pub-id-type="pmid">15154767</pub-id><pub-id pub-id-type="doi">10.1021/ci0342876</pub-id></citation></ref><ref id="B33"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Sugimoto</surname><given-names>N</given-names></name><name><surname>Nakano</surname><given-names>S</given-names></name><name><surname>Katoh</surname><given-names>M</given-names></name><name><surname>Matsumura</surname><given-names>A</given-names></name><name><surname>Nakamuta</surname><given-names>H</given-names></name><name><surname>Ohmichi</surname><given-names>T</given-names></name><name><surname>Yoneyama</surname><given-names>M</given-names></name><name><surname>Sasaki</surname><given-names>M</given-names></name></person-group><article-title>Thermodynamic parameters to predict stability of RNA/DNA hybrid duplexes</article-title><source>Biochemistry</source><year>1995</year><volume>34</volume><fpage>11211</fpage><lpage>11216</lpage><pub-id pub-id-type="pmid">7545436</pub-id></citation></ref><ref id="B34"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Sugimoto</surname><given-names>N</given-names></name><name><surname>Nakano</surname><given-names>S</given-names></name><name><surname>Yoneyama</surname><given-names>M</given-names></name><name><surname>Honda</surname><given-names>K</given-names></name></person-group><article-title>Improved thermodynamic parameters and helix initiation factor to predict stability of DNA duplexes</article-title><source>Nucleic Acids Res</source><year>1995</year><volume>24</volume><fpage>4501</fpage><lpage>4505</lpage><pub-id pub-id-type="doi">10.1093/nar/24.22.4501</pub-id></citation></ref><ref id="B35"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Walter</surname><given-names>AE</given-names></name><name><surname>Turner</surname><given-names>DH</given-names></name><name><surname>Kim</surname><given-names>J</given-names></name><name><surname>Lyttle</surname><given-names>MH</given-names></name><name><surname>Muller</surname><given-names>P</given-names></name><name><surname>Mathews</surname><given-names>DH</given-names></name><name><surname>Zuker</surname><given-names>M</given-names></name></person-group><article-title>Coaxial stacking of helixes enhances binding of oligoribonucleotides and improves predictions of RNA folding</article-title><source>Proc Natl Acad Sci, U S A</source><year>1994</year><volume>91</volume><fpage>9218</fpage><lpage>9222</lpage><pub-id pub-id-type="pmid">7524072</pub-id></citation></ref><ref id="B36"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Hofacker</surname><given-names>I</given-names></name><name><surname>Fontana</surname><given-names>W</given-names></name><name><surname>Stadler</surname><given-names>P</given-names></name><name><surname>Bonhoeffer</surname><given-names>S</given-names></name><name><surname>Tacker</surname><given-names>M</given-names></name><name><surname>Schuster</surname><given-names>P</given-names></name></person-group><article-title>Fast Folding and Comparison of RNA Secondary Structures</article-title><source>Monatshefte f Chemie</source><year>1994</year><volume>125</volume><fpage>167</fpage><lpage>188</lpage></citation></ref><ref id="B37"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Chalk</surname><given-names>AM</given-names></name><name><surname>Sonnhammer</surname><given-names>ELL</given-names></name></person-group><article-title>AOPredict An Antisense Oligonucleotide Prediction Program</article-title><year>2004</year><ext-link ext-link-type="uri" xlink:href="http://aopredict.cgb.ki.se/aodb.html"/><comment>Last visited September 1st 2004</comment></citation></ref><ref id="B38"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Blum</surname><given-names>A</given-names></name><name><surname>Langley</surname><given-names>P</given-names></name></person-group><article-title>Selection of relevant features and examples in machine learning</article-title><source>Artificial Intelligence</source><year>1998</year><volume>97</volume><fpage>245</fpage><lpage>271</lpage><pub-id pub-id-type="doi">10.1016/S0004-3702(97)00063-5</pub-id></citation></ref><ref id="B39"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Battiti</surname><given-names>R</given-names></name></person-group><article-title>Using Mutual Information for Selecting Features in supervised Neural Net Learning</article-title><source>IEEE Transactions on Neural Networks</source><year>1994</year><volume>5</volume></citation></ref><ref id="B40"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Vapnik</surname><given-names>VN</given-names></name></person-group><source>Statistical Learning Theory</source><year>1998</year><publisher-name>New York: John Wiley &#x00026; Sons</publisher-name></citation></ref><ref id="B41"><citation citation-type="book"><person-group person-group-type="author"><name><surname>M&#x000fc;ller</surname><given-names>KR</given-names></name><name><surname>Smola</surname><given-names>A</given-names></name><name><surname>R&#x000e4;tsch</surname><given-names>G</given-names></name><name><surname>Sch&#x000f6;lkopf</surname><given-names>B</given-names></name><name><surname>Kohlmorgen</surname><given-names>J</given-names></name><name><surname>Vapnik</surname><given-names>V</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Sch&#x000f6;lkopf B, Burges CJC, Smola AJ</surname></name></person-group><article-title>Predicting Time Series with Support Vector Machines</article-title><source>Advances in Kernel Methods &#x02013; Support Vector Learning</source><year>1999</year><publisher-name>Cambridge, MA: MIT Press</publisher-name><fpage>243</fpage><lpage>254</lpage></citation></ref><ref id="B42"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Sch&#x000f6;lkopf</surname><given-names>B</given-names></name><name><surname>Bartlett</surname><given-names>PL</given-names></name><name><surname>Smola</surname><given-names>A</given-names></name><name><surname>Williamson</surname><given-names>R</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Kearns MS, Solla SA, Cohn DA</surname></name></person-group><article-title>Shrinking the tube: a new support vector regression algorithm</article-title><source>Advances in Neural Information Processing Systems 11</source><year>1999</year><publisher-name>Cambridge, MA: MIT Press</publisher-name><fpage>330</fpage><lpage>336</lpage></citation></ref><ref id="B43"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Keerthi</surname><given-names>SS</given-names></name><name><surname>Lin</surname><given-names>CJ</given-names></name></person-group><article-title>Asymptotic behaviors of support vector machines with Gaussian kernel</article-title><source>Neural Computation</source><year>2003</year><volume>15</volume><fpage>1667</fpage><lpage>1689</lpage><pub-id pub-id-type="pmid">12816571</pub-id><pub-id pub-id-type="doi">10.1162/089976603321891855</pub-id></citation></ref><ref id="B44"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>HT</given-names></name><name><surname>Lin</surname><given-names>CJ</given-names></name></person-group><article-title>A study on sigmoid kernels for SVM and the training of non-PSD kernels by SMO-type methods</article-title><source>Tech rep, National Taiwan University, Department of Computer Science and Information Engineering</source><year>2003</year><ext-link ext-link-type="uri" xlink:href="http://www.csie.ntu.edu.tw/~cjlin/papers/tanh.pdf"/><comment>Revised Oct, 2003. Last access Oct, 2003</comment></citation></ref><ref id="B45"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Smola</surname><given-names>A</given-names></name><name><surname>Sch&#x000f6;lkopf</surname><given-names>B</given-names></name></person-group><article-title>Kernel Machines</article-title><year>2004</year><ext-link ext-link-type="uri" xlink:href="http://www.kernel-machines.org"/><comment>Last visited September 1st 2004</comment></citation></ref><ref id="B46"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Camps-Valls</surname><given-names>G</given-names></name><name><surname>Soria-Olivas</surname><given-names>E</given-names></name><name><surname>P&#x000e9;rez-Ruixo</surname><given-names>J</given-names></name><name><surname>Art&#x000e9;s-Rodr&#x000ed;guez</surname><given-names>A</given-names></name><name><surname>P&#x000e9;rez-Cruz</surname><given-names>F</given-names></name><name><surname>Figueiras-Vidal</surname><given-names>A</given-names></name></person-group><article-title>A Profile-Dependent Kernel-based Regression for Cyclosporine Concentration Prediction</article-title><source>Neural Information Processing Systems, NIPS, Vancouver, Canada</source><year>2001</year></citation></ref><ref id="B47"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Mart&#x000ed;n-Guerrero</surname><given-names>JD</given-names></name><name><surname>Camps-Valls</surname><given-names>G</given-names></name><name><surname>Soria-Olivas</surname><given-names>E</given-names></name><name><surname>Serrano-L&#x000f3;pez</surname><given-names>AJ</given-names></name><name><surname>P&#x000e9;rez-Ruixo</surname><given-names>JJ</given-names></name><name><surname>Jim&#x000e9;nez-Torres</surname><given-names>NV</given-names></name></person-group><article-title>Dosage Individualization of Erythropoietin using a Profile-Dependent Support Vector Regression</article-title><source>IEEE Transactions on Biomedical Engineering</source><year>2003</year><volume>50</volume><fpage>1136</fpage><lpage>1142</lpage><pub-id pub-id-type="pmid">14560766</pub-id><pub-id pub-id-type="doi">10.1109/TBME.2003.816084</pub-id></citation></ref><ref id="B48"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Camps-Valls</surname><given-names>G</given-names></name><name><surname>Chalk</surname><given-names>AM</given-names></name><name><surname>Serrano-L&#x000f3;pez</surname><given-names>AJ</given-names></name><name><surname>Mart&#x000ed;n-Guerrero</surname><given-names>JD</given-names></name><name><surname>Sonnhammer</surname><given-names>ELL</given-names></name></person-group><article-title>AOSVM &#x02013; AO design tool</article-title><year>2004</year><ext-link ext-link-type="uri" xlink:href="http://aosvm.cgb.ki.se/"/><comment>Last visited September 1st 2004</comment></citation></ref></ref-list><sec sec-type="display-objects"><title>Figures and Tables</title><fig position="float" id="F1"><label>Figure 1</label><caption><p>Illustration of Gaussian-like profiles for the penalization factor and the <italic>&#x003b5;</italic>-insensitive region in the P-SVR approach. In this case, we penalize harder the committed errors in the higher and lower efficacy regions. Additionally, the insensitive region becomes wider in medium AO efficacies, and thus few AOs will contribute to the cost function and, consequently, become support vectors. Only one additional parameter is introduced in the formulation, i.e. the width of the Gaussian profile, <italic>&#x003c3;</italic><sub><italic>P</italic></sub>.</p></caption><graphic xlink:href="1471-2105-5-135-1"/></fig><table-wrap position="float" id="T1"><label>Table 1</label><caption><p>Feature ranking using (a) the correlation coefficient between input features and efficacy (<graphic xlink:href="1471-2105-5-135-i17.gif"/>), (b) mutual information feature selection (MIFS) with <italic>&#x003b2; </italic>= 0.75, (c) SVM-based Recursive Feature Elimination (SVM-RFE), and (d) best selection in [21] using the correlation coefficient.</p></caption><table frame="hsides" rules="groups"><thead><tr><td></td><td align="center"><bold>FEATURE</bold></td><td align="center"><graphic xlink:href="1471-2105-5-135-i17.gif"/></td><td align="center"><bold>FEATURE</bold></td><td align="center"><bold>MI </bold>(<italic>&#x003b2; </italic>= 0.75)</td><td align="center"><bold>FEATURE</bold></td><td align="center"><bold>SVM-RFE </bold>||<bold>W</bold>||<sup>2</sup></td><td align="center"><bold>FEATURE</bold></td><td align="center"><graphic xlink:href="1471-2105-5-135-i17.gif"/><bold>in [21]</bold></td></tr></thead><tbody><tr><td align="center">1</td><td align="center">&#x00394;G</td><td align="center">-0.35</td><td align="center">&#x00394;G</td><td align="center">0.094</td><td align="center">&#x00394;H</td><td align="center">0.680</td><td align="center">GGGA</td><td align="center">0.26</td></tr><tr><td align="center">2</td><td align="center"># Cytosine</td><td align="center">0.31</td><td align="center"># Cytosine</td><td align="center">0.089</td><td align="center">&#x00394;S</td><td align="center">0.671</td><td align="center"># Cytosine</td><td align="center">0.23</td></tr><tr><td align="center">3</td><td align="center">TCCC</td><td align="center">0.28</td><td align="center">%GC content</td><td align="center">0.077</td><td align="center">&#x00394;G</td><td align="center">0.193</td><td align="center">&#x00394;H</td><td align="center">-0.19</td></tr><tr><td align="center">4</td><td align="center">5p&#x00394;G</td><td align="center">-0.26</td><td align="center">&#x00394;G/length</td><td align="center">0.075</td><td align="center"># Cytosine</td><td align="center">0.045</td><td align="center">&#x00394;G</td><td align="center">-0.18</td></tr><tr><td align="center">5</td><td align="center">&#x00394;H</td><td align="center">-0.24</td><td align="center">&#x00394;H</td><td align="center">0.064</td><td align="center">Hairpin quality</td><td align="center">0.035</td><td align="center">CAGT</td><td align="center">-0.18</td></tr><tr><td align="center">6</td><td align="center">&#x00394;H/length</td><td align="center">-0.22</td><td align="center">&#x00394;H/length</td><td align="center">0.061</td><td align="center"># Adenine</td><td align="center">0.024</td><td align="center">AGAG</td><td align="center">0.18</td></tr><tr><td align="center">7</td><td align="center">%GC content</td><td align="center">0.22</td><td align="center">&#x00394;S</td><td align="center">0.060</td><td align="center"># Thymine</td><td align="center">0.018</td><td align="center">GTGG</td><td align="center">0.17</td></tr><tr><td align="center">8</td><td align="center">CCCT</td><td align="center">0.21</td><td align="center"># Adenine</td><td align="center">0.043</td><td align="center">Hairpin length</td><td align="center">0.014</td><td align="center"># Guanine</td><td align="center">-0.15</td></tr><tr><td align="center">9</td><td align="center">CCAC</td><td align="center">0.21</td><td align="center"># Guanine</td><td align="center">0.042</td><td align="center">5p&#x00394;G</td><td align="center">0.009</td><td align="center">3p&#x00394;G</td><td align="center">0.14</td></tr><tr><td align="center">10</td><td align="center">CCCC</td><td align="center">0.21</td><td align="center">5p&#x00394;G</td><td align="center">0.040</td><td align="center">3p&#x00394;G</td><td align="center">0.005</td><td align="center">&#x00394;S</td><td align="center">-0.14</td></tr><tr><td align="center">11</td><td align="center">CTCT</td><td align="center">0.20</td><td align="center">Hairpin quality</td><td align="center">0.027</td><td align="center">Dimer</td><td align="center">0.004</td><td align="center">CCCC</td><td align="center">-0.13</td></tr><tr><td align="center">12</td><td align="center">CCCA</td><td align="center">0.20</td><td align="center">Hairpin length</td><td align="center">0.024</td><td align="center">Hairpin energy (Mfold)</td><td align="center">0.003</td><td align="center">Hairpin quality</td><td align="center">-0.11</td></tr><tr><td align="center">13</td><td align="center">ACAC</td><td align="center">-0.16</td><td align="center">Hairpin Energy</td><td align="center">0.022</td><td align="center"># Guanine</td><td align="center">0.001</td><td align="center">%GC content</td><td align="center">0.11</td></tr><tr><td align="center">14</td><td align="center"># Adenine</td><td align="center">-0.16</td><td align="center"># Thymine</td><td align="center">0.016</td><td align="center">Hairpin energy (vienna)</td><td align="center">0.000</td><td align="center">TGGC</td><td align="center">-0.10</td></tr></tbody></table></table-wrap><table-wrap position="float" id="T2"><label>Table 2</label><caption><p>Mean error (ME), root-mean-squared error (RMSE) and correlation coefficient (r) of models in the validation set. Success rates (SR) for efficacy higher than 0.75 or below 0.25 are also given for each feature selection method.</p></caption><table frame="hsides" rules="groups"><thead><tr><td align="center"><bold>Methods</bold></td><td align="center" colspan="5"><bold>SVR</bold></td><td align="center"><bold>P-SVR</bold></td></tr></thead><tbody><tr><td align="center"><bold>Selection</bold></td><td align="center"><bold>All feats.</bold></td><td align="center"><graphic xlink:href="1471-2105-5-135-i18.gif"/></td><td align="center">MI(<italic>&#x003b2; </italic>= 0.75)</td><td align="center"><bold>RFE</bold></td><td align="center"><bold>RFE</bold></td><td align="center"><bold>RFE</bold></td></tr><tr><td colspan="7"><hr></hr></td></tr><tr><td align="center"><bold>CV method</bold></td><td align="center">-</td><td align="center">-</td><td align="center">-</td><td align="center"><bold>8-fold</bold></td><td align="center"><bold>-RNA</bold></td><td align="center"><bold>-RNA</bold></td></tr><tr><td colspan="7"><hr></hr></td></tr><tr><td align="center"><bold>r</bold></td><td align="center">0.356</td><td align="center">0.367</td><td align="center">0.374</td><td align="center">0.398</td><td align="center">0.430</td><td align="center">0.440</td></tr><tr><td align="center"><bold>ME</bold></td><td align="center">-0.0280</td><td align="center">-0.0223</td><td align="center">-0.0104</td><td align="center">-0.0068</td><td align="center">0.031</td><td align="center">0.022</td></tr><tr><td align="center"><bold>RMSE</bold></td><td align="center">0.312</td><td align="center">0.300</td><td align="center">0.301</td><td align="center">0.299</td><td align="center">0.299</td><td align="center">0.278</td></tr><tr><td align="center"><bold>SR</bold><sub>&#x0003e;0.75</sub></td><td align="center">82.8</td><td align="center">87.5</td><td align="center">86.7</td><td align="center">87.5</td><td align="center">83.3</td><td align="center">83.3</td></tr><tr><td align="center"><bold>SR</bold><sub>&#x0003c;0.25</sub></td><td align="center">71.4</td><td align="center">73.9</td><td align="center">71.4</td><td align="center">73.9</td><td align="center">76.2</td><td align="center">82.9</td></tr></tbody></table></table-wrap></sec></back></article>



