<!DOCTYPE article PUBLIC "-//NLM//DTD Journal Archiving and Interchange DTD v2.3 20070202//EN" "archivearticle.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id><journal-title>BMC Bioinformatics</journal-title><issn pub-type="epub">1471-2105</issn><publisher><publisher-name>BioMed Central</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">15339346</article-id><article-id pub-id-type="pmc">PMC516800</article-id><article-id pub-id-type="publisher-id">1471-2105-5-118</article-id><article-id pub-id-type="doi">10.1186/1471-2105-5-118</article-id><article-categories><subj-group subj-group-type="heading"><subject>Methodology Article</subject></subj-group></article-categories><title-group><article-title>Estimating mutual information using B-spline functions &#x02013; an improved similarity measure for analysing gene expression data</article-title></title-group><contrib-group><contrib id="A1" corresp="yes" contrib-type="author"><name><surname>Daub</surname><given-names>Carsten O</given-names></name><xref ref-type="aff" rid="I1">1</xref><xref ref-type="aff" rid="I4">4</xref><email>carsten.daub@cgb.ki.se</email></contrib><contrib id="A2" contrib-type="author"><name><surname>Steuer</surname><given-names>Ralf</given-names></name><xref ref-type="aff" rid="I2">2</xref><email>steuer@agnld.uni-potsdam.de</email></contrib><contrib id="A3" contrib-type="author"><name><surname>Selbig</surname><given-names>Joachim</given-names></name><xref ref-type="aff" rid="I1">1</xref><email>selbig@mpimp-golm.mpg.de</email></contrib><contrib id="A4" contrib-type="author"><name><surname>Kloska</surname><given-names>Sebastian</given-names></name><xref ref-type="aff" rid="I1">1</xref><xref ref-type="aff" rid="I3">3</xref><email>kloska@scienion.de</email></contrib></contrib-group><aff id="I1"><label>1</label>Max Planck Institute of Molecular Plant Physiology, Potsdam, 14424, Germany</aff><aff id="I2"><label>2</label>Nonlinear Dynamics Group, Institute of Physics, University of Potsdam, Potsdam, 14415, Germany</aff><aff id="I3"><label>3</label>Scienion AG, Volmerstrasse 7a, Berlin, 12489, Germany</aff><aff id="I4"><label>4</label>Center for Genomics and Bioinformatics, Karolinska Institutet, Stockholm, 17177, Sweden</aff><pub-date pub-type="collection"><year>2004</year></pub-date><pub-date pub-type="epub"><day>31</day><month>8</month><year>2004</year></pub-date><volume>5</volume><fpage>118</fpage><lpage>118</lpage><ext-link ext-link-type="uri" xlink:href="http://www.biomedcentral.com/1471-2105/5/118"/><history><date date-type="received"><day>15</day><month>12</month><year>2003</year></date><date date-type="accepted"><day>31</day><month>8</month><year>2004</year></date></history><copyright-statement>Copyright &#x000a9; 2004 Daub et al; licensee BioMed Central Ltd.</copyright-statement><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/2.0"><p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/2.0"/>), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</p></license><abstract><sec><title>Background</title><p>The information theoretic concept of mutual information provides a general framework to evaluate dependencies between variables. In the context of the clustering of genes with similar patterns of expression it has been suggested as a general quantity of similarity to extend commonly used linear measures. Since mutual information is defined in terms of discrete variables, its application to continuous data requires the use of binning procedures, which can lead to significant numerical errors for datasets of small or moderate size.</p></sec><sec><title>Results</title><p>In this work, we propose a method for the numerical estimation of mutual information from continuous data. We investigate the characteristic properties arising from the application of our algorithm and show that our approach outperforms commonly used algorithms: The significance, as a measure of the power of distinction from random correlation, is significantly increased. This concept is subsequently illustrated on two large-scale gene expression datasets and the results are compared to those obtained using other similarity measures.</p><p>A C++ source code of our algorithm is available for non-commercial use from <email>kloska@scienion.de</email> upon request.</p></sec><sec><title>Conclusion</title><p>The utilisation of mutual information as similarity measure enables the detection of non-linear correlations in gene expression datasets. Frequently applied linear correlation measures, which are often used on an ad-hoc basis without further justification, are thereby extended.</p></sec></abstract></article-meta></front><body><sec><title>Background</title><p>The evaluation of the complex regulatory networks underlying molecular processes poses a major challenge to current research. With modern experimental methods in the field of gene expression, it is possible to monitor mRNA abundance for whole genomes [<xref ref-type="bibr" rid="B1">1</xref>,<xref ref-type="bibr" rid="B2">2</xref>]. To elucidate the functional relationships inherent in this data, a commonly used approach is the clustering of co-expressed genes [<xref ref-type="bibr" rid="B3">3</xref>]. In this context, the choice of the similarity measure used for clustering, as well as the clustering method itself, is crucial for the results obtained. Often, linear similarity measures such as the Euclidean distance or Pearson correlation are used in an ad-hoc manner. By doing so, it is possible that subsets of non-linear correlations contained in a given dataset are missed.</p><p>Therefore, information theoretic concepts, such as mutual information, are being used to extend more conventional methods in various contexts ranging from expression [<xref ref-type="bibr" rid="B4">4</xref>-<xref ref-type="bibr" rid="B8">8</xref>] and DNA sequence analysis [<xref ref-type="bibr" rid="B9">9</xref>,<xref ref-type="bibr" rid="B10">10</xref>], to reverse engineering [<xref ref-type="bibr" rid="B11">11</xref>] and independent component analysis [<xref ref-type="bibr" rid="B12">12</xref>,<xref ref-type="bibr" rid="B13">13</xref>]. Also aside the bioinformatics field, mutual information is widely utilised in diverse disciplines, such as physics [<xref ref-type="bibr" rid="B14">14</xref>], image recognition [<xref ref-type="bibr" rid="B15">15</xref>], speech recognition [<xref ref-type="bibr" rid="B16">16</xref>], and various others. In extension to other similarity measures, mutual information provides a general measure of statistical dependence between variables. It is thereby able to detect any type of functional relationship, extending the potentialities of linear measures as illustrated in Figure <xref ref-type="fig" rid="F1">1</xref>.</p><p>In this work, we discuss mutual information as a measure of similarity between variables. In the first section, we give a short introduction into the basic concepts including a brief description of the commonly used approaches for numerical estimation from continuous data. In the following section, we then present an algorithm for estimating mutual information from finite data.</p><p>The properties arising from this approach are compared to previously existing algorithms. In subsequent sections, we then apply our concept to large-scale cDNA abundance datasets and determine if these datasets can be sufficiently described using linear measurements or if a significant amount of non-linear correlations are missed.</p><sec><title>Mutual information</title><p>Mutual information represents a general information theoretic approach to determine the statistical dependence between variables. The concept was initially developed for discrete data. For a system, <italic>A</italic>, with a finite set of <italic>M </italic>possible states {<italic>a</italic><sub>1</sub>, <italic>a</italic><sub>2</sub>, ... , <inline-graphic xlink:href="1471-2105-5-118-i1.gif"/>}, the Shannon entropy <italic>H</italic>(<italic>A</italic>) is defined as [<xref ref-type="bibr" rid="B17">17</xref>]</p><p><inline-graphic xlink:href="1471-2105-5-118-i2.gif"/></p><p>where <italic>p</italic>(<italic>a<sub>i</sub></italic>) denotes the probability of the state <italic>a<sub>i</sub></italic>. The Shannon entropy is a measure for how evenly the states of <italic>A </italic>are distributed. The entropy of system <italic>A </italic>becomes zero if the outcome of a measurement of <italic>A </italic>is completely determined to be <italic>a<sub>j</sub></italic>, thus if <italic>p</italic>(<italic>a<sub>j</sub></italic>) = 1 and <italic>p</italic>(<italic>a<sub>i</sub></italic>) = 0 for all <italic>i </italic>&#x02260; <italic>j</italic>, whereas the entropy becomes maximal if all probabilities are equal. The joint entropy <italic>H</italic>(<italic>A, B</italic>) of two systems <italic>A </italic>and <italic>B </italic>is defined analogously</p><p><inline-graphic xlink:href="1471-2105-5-118-i3.gif"/></p><p>This leads to the relation</p><p><italic>H</italic>(<italic>A, B</italic>) &#x02264; <italic>H</italic>(<italic>A</italic>) + <italic>H</italic>(<italic>B</italic>) &#x000a0;&#x000a0;&#x000a0; (3)</p><p>which fulfils equality only in the case of statistical independence of <italic>A </italic>and <italic>B</italic>. Mutual information <italic>MI</italic>(<italic>A, B</italic>) can be defined as [<xref ref-type="bibr" rid="B17">17</xref>]</p><p><italic>MI</italic>(<italic>A, B</italic>) = <italic>H</italic>(<italic>A</italic>) + <italic>H</italic>(<italic>B</italic>) - <italic>H</italic>(<italic>A, B</italic>) &#x02265; 0 &#x000a0;&#x000a0;&#x000a0; (4)</p><p>It is zero if <italic>A </italic>and <italic>B </italic>are statistically independent and increases the less statistically independent <italic>A </italic>and <italic>B </italic>are.</p><p>If mutual information is indeed to be used for the analysis of gene-expression data, the continuous experimental data need to be partitioned into discrete intervals, or bins. In the following section, we briefly review the established procedures; a description of how we have extended the basic approach will be provided in the subsequent section.</p></sec><sec><title>Estimates from continuous data</title><p>In the case of discrete data the estimation of the probabilities <italic>p</italic>(<italic>a<sub>i</sub></italic>) is straightforward. Many practical applications, however, supply continuous data for which the probability distributions are unknown and have to be estimated. In a widely used approach [<xref ref-type="bibr" rid="B7">7</xref>], the calculation of mutual information is based on the binning of data into <italic>M </italic>discrete intervals <italic>a<sub>i</sub></italic>, <italic>i </italic>= 1... <italic>M<sub>A</sub></italic>. For experimental data consisting of <italic>N </italic>measurements of a variable <italic>x<sub>u</sub></italic>, <italic>u </italic>= 1... <italic>N</italic>, an indicator function &#x00398;<sub><italic>i </italic></sub>counts the number of data points within each bin. The probabilities are then estimated based on the relative frequencies of occurrence</p><p><inline-graphic xlink:href="1471-2105-5-118-i4.gif"/></p><p>with</p><p><inline-graphic xlink:href="1471-2105-5-118-i5.gif"/></p><p>For two variables the joint probabilities <inline-graphic xlink:href="1471-2105-5-118-i6.gif"/> are calculated analogously from a multivariate histogram. Additionally it has been suggested [<xref ref-type="bibr" rid="B14">14</xref>] to adaptively choose the sizes of the bins, so that each bin constructed nearly has a uniform distribution of points. In a different approach, kernel methods are used for the estimation of the probability density of Eq. (5) [<xref ref-type="bibr" rid="B18">18</xref>-<xref ref-type="bibr" rid="B20">20</xref>]. Entropies are then calculated by integration of the estimated densities. Recently, an entropy estimator <inline-graphic xlink:href="1471-2105-5-118-i7.gif"/> was suggested [<xref ref-type="bibr" rid="B21">21</xref>] and showed in an extensive comparison to other commonly used estimators to be superior.</p></sec></sec><sec><title>Results</title><sec><title>Fuzzy mutual information</title><p>In the classical binning approach, described above, each data point is assigned to one, and only one, bin. For data points near to the border of a bin, small fluctuations due to biological or measurement noise might shift these points to neighbouring bins. Especially for datasets of moderate size, the positions of the borders of the bins can thereby strongly affect the resulting mutual information [<xref ref-type="bibr" rid="B18">18</xref>]. In a manner analogous to kernel density estimators (KDE), we now present a generalisation to the classical binning in which we aim to overcome some of the drawbacks associated with the simple approach. Within our algorithm, we allow the data points to be assigned to several bins simultaneously. For this, we extended the indicator function &#x00398;(<italic>x</italic>) to the set of polynomial B-spline functions. Here, we do not provide the mathematical details for these functions since they have been discussed extensively in the literature [<xref ref-type="bibr" rid="B22">22</xref>-<xref ref-type="bibr" rid="B24">24</xref>], but rather focus on the practical applicability. Within the B-spline approach, each measurement is assigned to more than one bin, <italic>i</italic>, with weights given by the B-spline functions <italic>B</italic><sub><italic>i,k</italic></sub>. The spline order <italic>k </italic>determines the shape of the weight functions and thereby the number of bins each of the data points is assigned to. A spline order <italic>k </italic>= 1 corresponds to the simple binning, as described in the previous section: Each data point is assigned to exactly one bin (Figure <xref ref-type="fig" rid="F2">2</xref>, left). For <italic>k </italic>= 3, each data point is assigned to three bins, with the respective weights given by the values of the B-spline functions at the data point (Figure <xref ref-type="fig" rid="F2">2</xref>, right).</p></sec><sec><title>B-spline functions</title><p>The first step in the definition of the B-spline functions is the definition of a knot vector <italic>t</italic><sub><italic>i </italic></sub>for a number of bins <italic>i </italic>= 1... <italic>M </italic>and one given spline order <italic>k </italic>= 1... <italic>M </italic>- 1 [<xref ref-type="bibr" rid="B22">22</xref>]</p><p><inline-graphic xlink:href="1471-2105-5-118-i8.gif"/></p><p>where the spline order determines the degree of the polynomial functions. The domain of the B-spline functions lies in the interval <italic>z </italic>&#x02208; [0, <italic>M </italic>- <italic>k </italic>+ 1]. To cover the range of the variables, the new indicator function based on the B-spline functions needs to be linearly transformed to map their range. The recursive definition of the B-spline functions are as follows [<xref ref-type="bibr" rid="B22">22</xref>]</p><p><inline-graphic xlink:href="1471-2105-5-118-i9.gif"/></p><p>An important property of B-spline functions is the implicit standardisation of coefficients: All weights belonging to one data point sum up to unity.</p></sec><sec><title>Algorithm</title><sec><title>Input</title><p>&#x02022; Variables <italic>x </italic>and <italic>y </italic>with values <italic>x</italic><sub><italic>u </italic></sub>and <italic>y</italic><sub><italic>u</italic></sub>, <italic>u </italic>= 1... <italic>N</italic></p><p>&#x02022; Bins <italic>a</italic><sub><italic>i</italic></sub>, <italic>i </italic>= 1... <italic>M</italic><sub><italic>x </italic></sub>and <italic>b</italic><sub><italic>j</italic></sub>, <italic>j </italic>= 1... <italic>M</italic><sub><italic>y</italic></sub></p><p>&#x02022; Spline order <italic>k</italic></p></sec><sec><title>Output</title><p>&#x02022; Mutual information between variable <italic>x </italic>and <italic>y</italic></p></sec><sec><title>Algorithm</title><p>1. Calculation of marginal entropy for variable <italic>x</italic></p><p>(a) Determine <inline-graphic xlink:href="1471-2105-5-118-i10.gif"/> with</p><p><inline-graphic xlink:href="1471-2105-5-118-i11.gif"/></p><p>(b) Determine <italic>M</italic><sub><italic>x </italic></sub>weighting coefficients for each <italic>x</italic><sub><italic>u </italic></sub>from <inline-graphic xlink:href="1471-2105-5-118-i12.gif"/></p><p>(c) Sum over all <italic>x</italic><sub><italic>u </italic></sub>and determine <italic>p</italic>(<italic>a</italic><sub><italic>i</italic></sub>) for each bin <italic>a</italic><sub><italic>i </italic></sub>from</p><p><inline-graphic xlink:href="1471-2105-5-118-i13.gif"/></p><p>(d) Determine entropy <italic>H</italic>(<italic>x</italic>) according to Eq. (1)</p><p>2. Calculation of joint entropy of two variables <italic>x </italic>and <italic>y</italic></p><p>(a) Apply steps 1 (a) and (b) to both variables <italic>x </italic>and <italic>y</italic>, independently</p><p>(b) Calculate joint probabilities <italic>p</italic>(<italic>a</italic><sub><italic>i</italic></sub>, <italic>b</italic><sub><italic>j</italic></sub>) for all <italic>M</italic><sub><italic>x </italic></sub>&#x000d7; <italic>M</italic><sub><italic>y </italic></sub>bins according to</p><p><inline-graphic xlink:href="1471-2105-5-118-i14.gif"/></p><p>(c) Calculate the joint entropy <italic>H</italic>(<italic>x,y</italic>) according to Eq. (2)</p><p>3. Calculate the mutual information <italic>MI</italic>(<italic>x,y</italic>) according to Eq. (4)</p></sec></sec><sec><title>Example</title><p>We show the estimation with the standard binning and our approach ex-emplarily on two artificial variables <italic>x </italic>= 0.0,0.2,0.4,0.6,0.8,1.0 and <italic>y </italic>= 0.8,1.0,0.6,0.4,0.0,0.2 for <italic>M </italic>= 3 bins, spline order <italic>k </italic>= 2, and the logarithm to basis two.</p><sec><title>Simple binning</title><p>For both variables, each of the three histogram bins contains two values <italic>p</italic>(<italic>a</italic><sub>1</sub>) = <italic>p</italic>(<italic>a</italic><sub>2</sub>) = <italic>p</italic>(<italic>a</italic><sub>3</sub>) = <inline-graphic xlink:href="1471-2105-5-118-i15.gif"/>, analogously for <italic>p</italic>(<italic>b<sub>i</sub></italic>) due to the symmetry of data <italic>H</italic>(<italic>x</italic>) = <italic>H</italic>(<italic>y</italic>) = <inline-graphic xlink:href="1471-2105-5-118-i16.gif"/> = log<sub>2 </sub>3 &#x02248; 1.58. For the calculation of the joint probability, three of the nine two dimensional bins contain two values each <italic>p</italic>(<italic>a</italic><sub>1</sub>, <italic>b</italic><sub>3</sub>) = <italic>p</italic>(<italic>a</italic><sub>2</sub>, <italic>b</italic><sub>2</sub>) = <italic>p</italic>(<italic>a</italic><sub>3</sub>, <italic>b</italic><sub>1</sub>) = <inline-graphic xlink:href="1471-2105-5-118-i15.gif"/> resulting in <italic>H</italic>(<italic>x, y</italic>) = log<sub>2 </sub>3 and <italic>MI</italic>(<italic>x, y</italic>) = log<sub>2 </sub>3.</p></sec><sec><title>B-spline approach</title><p>The modified indicator function <inline-graphic xlink:href="1471-2105-5-118-i17.gif"/> is determined to <italic>B</italic><sub><italic>i,k</italic></sub>(2<italic>x</italic>) according to Eq. (9) (rule 1(a)). For each value <italic>x</italic><sub><italic>u </italic></sub>three weighting coefficients are determined (rule 1(c)) and probabilities are calculated (rule 1(d)) (Table <xref ref-type="table" rid="T1">1</xref>). The analogous procedure is applied to variable <italic>y </italic>and the single entropies are calculated to <italic>H</italic>(<italic>x</italic>) = <italic>H</italic>(<italic>y</italic>) = Iog<sub>2</sub>(10) - 0.61og<sub>2</sub>(3) - 0.41og<sub>2</sub>(4) &#x02248; 1.57. Both, <italic>H</italic>(<italic>A</italic>) and <italic>H</italic>(<italic>B</italic>), are slightly smaller than the entropies calculated from the simple binning. The joint probabilities are <italic>p</italic>(<italic>a</italic><sub>1</sub>, <italic>b</italic><sub>1</sub>) = <italic>p</italic>(<italic>a</italic><sub>3</sub>, <italic>b</italic><sub>3</sub>) = 0, <italic>p</italic>(<italic>a</italic><sub>1</sub>, <italic>b</italic><sub>2</sub>) = <italic>p</italic>(<italic>a</italic><sub>2</sub>, <italic>b</italic><sub>1</sub>) = <italic>p</italic>(<italic>a</italic><sub>2</sub>, <italic>b</italic><sub>3</sub>) = <italic>p</italic>(<italic>a</italic><sub>3</sub>, <italic>b</italic><sub>2</sub>) = 0.56/6, <italic>p</italic>(<italic>a</italic><sub>1</sub>, <italic>b</italic><sub>3</sub>) = <italic>p</italic>(<italic>a</italic><sub>3</sub>, <italic>b</italic><sub>1</sub>) = 1.24/6, <italic>p</italic>(<italic>a</italic><sub>2</sub>, <italic>b</italic><sub>2</sub>) = 1.28/6 (rule 2 (b)) resulting in <italic>H</italic>(<italic>x,y</italic>) = 2.7 and <italic>MI</italic>(<italic>x,y</italic>) = 0.45.</p><p>In the next sections, we discuss some of the properties arising from the utilisation of B-spline functions for the estimation of mutual information and compare our approach to other commonly used estimators. We support this discussion using examples for which the underlying distributions and thereby the true mutual information is known.</p></sec></sec><sec><title>Size of data</title><p>It has been discussed elsewhere [<xref ref-type="bibr" rid="B25">25</xref>-<xref ref-type="bibr" rid="B28">28</xref>,<xref ref-type="bibr" rid="B20">20</xref>] that the estimated mutual information is systematically overestimated for a finite size of <italic>N </italic>data points. For the simple binning approach, the mean observed mutual information can be calculated explicitly as the deviation from the true mutual information</p><p><inline-graphic xlink:href="1471-2105-5-118-i18.gif"/></p><p>As can be seen for an example of artificially generated equidistributed random numbers (Figure <xref ref-type="fig" rid="F3">3</xref>, left), mutual information calculated from the simple binning scales linearly with 1/<italic>N</italic>, with the slope depending on the number of bins <italic>M </italic>in accordance with Eq. (12). Figure <xref ref-type="fig" rid="F3">3</xref> shows that this scaling is preserved for the extension to B-spline functions, while the slope is significantly decreased for <italic>k </italic>= 3, compared to the estimation with the simple binning (<italic>k </italic>= 1). Mutual information calculated from KDE does not show a linear behaviour but rather an asymptotic one with a linear tail for large datasets. The values are slightly increased compared to the ones from the B-spline approach. The entropy estimator <inline-graphic xlink:href="1471-2105-5-118-i7.gif"/> gives values comparable to the ones obtained from the B-spline approach.</p><p>More importantly, a similar result also holds for the standard deviation of mutual information. As shown in Figure <xref ref-type="fig" rid="F3">3</xref> (right), the standard deviation of the mutual information estimated with the simple binning (<italic>k </italic>= 1) scales with 1/<italic>N </italic>for statistically independent events [<xref ref-type="bibr" rid="B26">26</xref>,<xref ref-type="bibr" rid="B29">29</xref>]. For the B-spline approach (<italic>k </italic>= 3), this scaling still holds, but the average values are decreased significantly. For the KDE approach, an asymptotic run above the values from the B-spline approach is observed, again with linear tail for large datasets. <inline-graphic xlink:href="1471-2105-5-118-i7.gif"/> shows a linear scaling slightly below the simple binning.</p></sec><sec><title>The spline order</title><p>The interpretation of any results obtained from the application of mutual information to experimental data is based on testing to see if the calculated results are consistent with a previously chosen null hypothesis. By following the intuitive approach that the null hypothesis assumes the statistical independence of variables, mutual information is tested against a surrogate dataset, which is consistent with this null hypothesis. As discussed previously in more detail [<xref ref-type="bibr" rid="B20">20</xref>], one way of generating such a surrogate dataset is by random permutations of the original data. From the mutual information of the original dataset <italic>MI</italic>(<italic>X,Y</italic>)<sup>data</sup>, the average value obtained from surrogate data &#x0003c;<italic>MI</italic>(<italic>X</italic><sup>surr</sup>, <italic>Y</italic><sup>surr</sup>) &#x0003e;, and its standard deviation &#x003c3;<sup>surr</sup>, the significance <italic>S </italic>can be formulated as</p><p><inline-graphic xlink:href="1471-2105-5-118-i19.gif"/></p><p>For each <italic>S </italic>the null hypothesis can be rejected to a certain level &#x003b1; depending on the underlying distribution. With increasing significance the probability of false positive associations drops.</p><p>In the following, we address the influence of the spline order and the number of bins on the estimation of mutual information. Based on 300 data points of an artificially-generated dataset drawn from the distribution shown in Figure <xref ref-type="fig" rid="F1">1</xref>, we calculate the mutual information for <italic>M </italic>= 6 bins and different spline orders <italic>k </italic>= 1... 5 (Figure <xref ref-type="fig" rid="F4">4</xref>, left).</p><p>From 300 shuffled realisations of this dataset, the mean and maximum mutual information are shown with the standard deviation as error-bars. For all spline orders the null hypothesis can be rejected, in accordance with the dataset shown in Figure <xref ref-type="fig" rid="F1">1</xref>. To estimate the strength of the rejection, we calculate the significance according to Eq. (13) (Figure <xref ref-type="fig" rid="F4">4</xref>, right). It can be observed that the largest change in the significance of the mutual information occurs in the transition from <italic>k </italic>= 1 (simple boxes) to <italic>k </italic>= 2 with an increase by roughly two-fold. Using more sophisticated functions (<italic>k </italic>&#x02265; 3) does not further improve the significance. Similar findings have been reported in the context of kernel density estimators [<xref ref-type="bibr" rid="B19">19</xref>]. The major contribution leading to this increase of the significance is given by the distribution of surrogate data which becomes more narrow for <italic>k </italic>&#x0003e; 1 leading to smaller standard deviations &#x003c3;<sup>surr</sup>.</p><p>The same dataset is used to show the dependency of mutual information on the number of bins for two spline orders <italic>k </italic>= 1 and <italic>k </italic>= 3 (Figure <xref ref-type="fig" rid="F5">5</xref>). Mutual information estimated from data as well as from surrogate data shows a robust run without strong fluctuations within the range of bins shown. From this we can conclude that the choice of the number of bins does not affect the resulting mutual information notably as long as it is chosen to be within a reasonable range.</p><p>Again, the significance is calculated (Figure <xref ref-type="fig" rid="F6">6</xref>) and compared to the significances obtained from the KDE approach and the <inline-graphic xlink:href="1471-2105-5-118-i7.gif"/> estimator. It can be observed that the significance of the mutual information calculated with B-spline functions increased roughly by two-fold compared to the simple binning. The significance obtained from KDE is not depending on <italic>M </italic>and was determined to be similar to the significance estimated from the B-spline approach. The numerically expensive integration of KDE, however, limits the size of utilisable datasets. The KDE run time requirements were <inline-graphic xlink:href="1471-2105-5-118-i20.gif"/>(10<sup>4</sup>) times higher than the ones from the B-spline approach. Strategies to simplify the integration step were proposed [<xref ref-type="bibr" rid="B20">20</xref>] but have to be used with caution since they assume particular properties of the distribution of experimental data that are in general not fulfilled. The recently introduced entropy estimator <inline-graphic xlink:href="1471-2105-5-118-i7.gif"/> produces intermediate significances between the ones from the binning and the B-spline approach for higher bin numbers. For low bin numbers, the significances are relatively poor.</p></sec><sec><title>Application on data</title><p>We now turn to the analysis of experimentally measured gene expression data. As shown previously, the application of mutual information to large-scale expression data reveals biologically-relevant clusters of genes [<xref ref-type="bibr" rid="B7">7</xref>,<xref ref-type="bibr" rid="B30">30</xref>]. In this section, we will not repeat these analyses, but determine if the correlations detected using mutual information are missed using the established linear measures.</p><p>Among the most frequently used measures of similarity for clustering co-expressed genes are the Euclidean distance and the Pearson correlation coefficient <italic>R </italic>[<xref ref-type="bibr" rid="B3">3</xref>]. If correlations are well described by the Pearson correlation and the distribution of data is approximately Gaussian like, the relationship between the mutual information and the Pearson correlation given by [<xref ref-type="bibr" rid="B32">32</xref>]</p><p><inline-graphic xlink:href="1471-2105-5-118-i21.gif"/></p><p>is expected to be fulfilled. Therefore, we calculated both, the mutual information and the Pearson correlation, for two large-scale gene expression datasets (Figure <xref ref-type="fig" rid="F7">7</xref>). For each pair of genes <italic>X </italic>and <italic>Y </italic>we plot the tuple (<italic>MI</italic>(<italic>X,Y</italic>), <italic>R</italic>(<italic>X,Y</italic>)). In order to address significance, we additionally calculate all tuples from shuffled data.</p><p>The first dataset contains cDNA measurements for <italic>S. cerevisiae </italic>for up to <italic>E</italic><sub>1 </sub>= 300 experiments [<xref ref-type="bibr" rid="B31">31</xref>]. To avoid numerical effects arising from different numbers of defined expression values (missing data points) for each gene, we exclusively utilised genes that are fully defined for all experimental conditions resulting in <italic>G</italic><sub>1 </sub>= 5345 genes. Analysis on this dataset using mutual information has been done before [<xref ref-type="bibr" rid="B20">20</xref>,<xref ref-type="bibr" rid="B32">32</xref>] on rank-ordered data. The rank-ordering lead to homogeneously distributed data and thereby enabled the application of a simplified algorithm for the numerical estimation from kernel density estimators. The utilisation of our B-spline approach allows us to extend this analysis to non rank-ordered data thereby keeping the original distribution of experimental data. In contrast to the previous studies we find for non rank-ordered data that the theoretical prediction of Eq. 14 is no longer a lower bound for the comparison. Many tuples with high Pearson correlation but low mutual information can be detected arising from outlying expression values (Figure <xref ref-type="fig" rid="F8">8A</xref>). However, pairs of genes with high mutual information and low Pearson correlation, thus indicating a non-linear correlation, are not observed. The only remarkable tuple (marked with an arrow in Figure <xref ref-type="fig" rid="F7">7</xref> and shown in Figure <xref ref-type="fig" rid="F8">8B</xref>) also arises from outlying values.</p><p>The second dataset contains cDNA measurements for <italic>E</italic><sub>2 </sub>= 102 experiments on G<sub>2 </sub>= 22608 genes derived from 20 different human tissues [<xref ref-type="bibr" rid="B33">33</xref>]. In contrast to the first dataset, tuples with low Pearson correlation but high mutual information are indeed detected. For two exemplary chosen tuples (Figure <xref ref-type="fig" rid="F8">8C</xref> and <xref ref-type="fig" rid="F8">8D</xref>), clusters of experimental conditions can be clearly detected by eye. Such type of correlations are missed by analyses based exclusively on linear measures, such as the the analysis done in the original publication of this dataset.</p><p>For both datasets, tuples calculated from shuffled data (Figure <xref ref-type="fig" rid="F7">7</xref>, blue data points) result in small values for both similarity measures. Thereby, they indicate a high significance of the original associations. Peaks with high Pearson correlation in the first dataset arise from gene-gene associations with outlying values. Significance values for the exemplarily chosen pairs of genes of the second dataset (Figure <xref ref-type="fig" rid="F8">8C</xref>, and <xref ref-type="fig" rid="F8">8D</xref>) were explicitly calculated (Figure <xref ref-type="fig" rid="F9">9</xref>). They show high significance values for the two examples of observed non-linear correlations on the basis of the mutual information. Compared to this, the significances calculated from the Pearson correlation are poor. In summary, our analysis confirms for the first dataset that the Pearson correlation does not miss any non-linear correlations. As a side effect we are able to detect gene-gene pairs containing outlying values. For the second dataset, however, a substantial amount of non-linear correlations was detected. Gene-gene pairs exemplarily chosen from this fraction show a clustering of data points (experiments) with a high significance. Even though such patterns can be easily found by eye, computational methods need to be applied for the inspection of several hundred million comparisons.</p></sec></sec><sec><title>Discussion and conclusion</title><p>After a brief introduction into the information theoretic concept of mutual information, we proposed a method for its estimation from continuous data. Within our approach, we extend the bins of the classical algorithm to polynomial B-spline functions: Data points are no longer assigned to exactly one bin but to several bins simultaneously, with weights given by the B-spline functions. By definition, the weighting coefficients for each data point automatically sum up to unity. Though our algorithm is reminiscent of kernel density estimators [<xref ref-type="bibr" rid="B18">18</xref>], it keeps the basic idea to associate data points to discrete bins. In this way, we are able to avoid time-consuming numerical integration steps usually intrinsic to estimates of mutual information using kernel density estimators [<xref ref-type="bibr" rid="B20">20</xref>].</p><p>To show that our approach improves the simple binning method and to compare it to KDE and the recently reported estimator <inline-graphic xlink:href="1471-2105-5-118-i7.gif"/>, we provided a systematic comparison between all these algorithms for artificially generated datasets, drawn from a known distribution. We found that mutual information, as well as its standard deviation, scales linearly with the inverse size of a dataset for the standard binning method, for the B-spline approach, and for <inline-graphic xlink:href="1471-2105-5-118-i7.gif"/>. For the KDE approach we find an asymptotic behaviour with a linear tail for large datasets. Moreover, the discrimination of correlations from the hypothesis of statistical independence is significantly improved by extending the standard binning method to B-spline functions, as shown by a two-fold increase of the significance. Compared to KDE, the B-spline functions produce similar significances. However, due to the numerical expenses of the KDE, an application of this algorithm is limited to datasets of mod-erate size. The application of <inline-graphic xlink:href="1471-2105-5-118-i7.gif"/> leads to significances in-between the standard binning and the B-spline approach for reasonable bin numbers. Linear correlation measures are among the most applied measures of similarity in the literature. Often, they are used on an ad-hoc basis and it is unclear whether a considerable number of non-linear correlations are missed. Here, we asked the question whether previous analyses, based on linear correlations, sufficiently described the correlations within gene expression datasets or whether mutual information detects additional correlations that are not detected by linear measures, such as the Pearson correlation. For data that is well described by the Pearson correlation, we can give the relation of the Pearson correlation to the mutual information explicitly [<xref ref-type="bibr" rid="B32">32</xref>]. Both measures were then applied to publicly available large-scale gene expression datasets [<xref ref-type="bibr" rid="B31">31</xref>,<xref ref-type="bibr" rid="B33">33</xref>]. We aimed to verify whether non-linear correlations shown as deviations from this relation can be detected.</p><p>Our findings show that the first dataset is fairly well described by the given relation of the Pearson correlation to the mutual information. No data points with high mutual information and low Pearson correlation are detected. Comparisons of genes containing outlying values, however, result in deviations with low mutual information and high Pearson correlation. From this, it follows that previous analyses on this dataset, based on Pearson correlation, did not miss any non-linear correlations. This presents an important finding since it is by all means supposable that the regulations inherent in the genetic network under consideration might show more complex behaviour than the observed linear ones. Even for one of the largest expression datasets at hand, insufficient data might complicate the detection of such complex patterns of regulation. Alternatively, the biological mechanisms which underlay the regulatory networks might not lead to non-linear correlations. It also has to be considered that the experimental methods applied for the generation of this dataset may make non-linear correlations difficult to detect. The second dataset, in contrast, reveals highly significant tuples with high mutual information and low Pearson correlation. Detailed gene-gene plots of such tuples show that the expression values of the contributing genes fall into groups of experimental conditions. Without attempting to draw conclusions about the biological context of such clusters here, they might reflect interesting situations worth to be analysed in detail.</p></sec><sec><title>Authors' contributions</title><p>Most of the manuscript text was written by CD and edited by all authors. CD carried out the calculations and produced the figures. RS strongly contributed to the theoretical background of entropy and mutual information.</p><p>The implementation of the C++ program was carried out by SK. JS and SK supervised this work. All authors read and approved the final manuscript.</p></sec></body><back><ack><sec><title>Acknowledgements</title><p>The authors would like to thank Joachim Kopka and Janko Weise for stimulating discussions and Megan McKenzie for editing the manuscript (all of the MPI-MP). RS acknowledges financial support by the HSP-N grant of the the state of Brandenburg.</p></sec></ack><ref-list><ref id="B1"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Schena</surname><given-names>M</given-names></name><name><surname>Shalon</surname><given-names>D</given-names></name><name><surname>Davis</surname><given-names>RW</given-names></name><name><surname>Brown</surname><given-names>PO</given-names></name></person-group><article-title>Quantitative Monitoring of Gene Expression Patterns with a Complementary DNA Microarray</article-title><source>Science</source><year>1995</year><volume>270</volume><fpage>467</fpage><lpage>470</lpage><pub-id pub-id-type="pmid">7569999</pub-id></citation></ref><ref id="B2"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Velculescu</surname><given-names>VE</given-names></name><name><surname>Zhang</surname><given-names>I</given-names></name><name><surname>Vogelstein</surname><given-names>B</given-names></name><name><surname>Kinzler</surname><given-names>K</given-names></name></person-group><article-title>Serial Analysis of Gene Expression</article-title><source>Science</source><year>1995</year><volume>270</volume><fpage>484</fpage><lpage>487</lpage><pub-id pub-id-type="pmid">7570003</pub-id></citation></ref><ref id="B3"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Eisen</surname><given-names>MB</given-names></name><name><surname>Spellman</surname><given-names>PT</given-names></name><name><surname>Brown</surname><given-names>PO</given-names></name><name><surname>Botstein</surname><given-names>D</given-names></name></person-group><article-title>Cluster analysis and display of genome-wide expression patterns</article-title><source>Proc Natl Acad Sci USA</source><year>1998</year><volume>95</volume><fpage>14863</fpage><lpage>14868</lpage><pub-id pub-id-type="pmid">9843981</pub-id><pub-id pub-id-type="doi">10.1073/pnas.95.25.14863</pub-id></citation></ref><ref id="B4"><citation citation-type="other"><person-group person-group-type="author"><name><surname>D'haeseleer</surname><given-names>P</given-names></name><name><surname>Weng</surname><given-names>X</given-names></name><name><surname>Fuhrman</surname><given-names>S</given-names></name><name><surname>Somogyi</surname><given-names>R</given-names></name></person-group><article-title>Information processing in cells and tissues</article-title><source>Plenum Publishing</source><year>1997</year><fpage>203</fpage><lpage>212</lpage><ext-link ext-link-type="uri" xlink:href="http://www.cs.unm.edu/~patrik/networks/IPCAT/ipcat.html"/></citation></ref><ref id="B5"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>D'haeseleer</surname><given-names>P</given-names></name><name><surname>Liang</surname><given-names>S</given-names></name><name><surname>Somogyi</surname><given-names>R</given-names></name></person-group><article-title>Genetic network inference: from co-expression clustering to reverse engineering</article-title><source>Bioinformatics</source><year>2000</year><volume>16</volume><fpage>707</fpage><lpage>726</lpage><pub-id pub-id-type="pmid">11099257</pub-id><pub-id pub-id-type="doi">10.1093/bioinformatics/16.8.707</pub-id></citation></ref><ref id="B6"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Michaels</surname><given-names>GS</given-names></name><name><surname>Carr</surname><given-names>DB</given-names></name><name><surname>Askenazi</surname><given-names>M</given-names></name><name><surname>Fuhrmann</surname><given-names>S</given-names></name><name><surname>Wen</surname><given-names>X</given-names></name><name><surname>Somogyi</surname><given-names>R</given-names></name></person-group><article-title>Cluster analysis and data visualization of large-scale gene expression data</article-title><source>Pac Symp Biocomput</source><year>1998</year><fpage>42</fpage><lpage>53</lpage><pub-id pub-id-type="pmid">9697170</pub-id></citation></ref><ref id="B7"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Butte</surname><given-names>AJ</given-names></name><name><surname>Kohane</surname><given-names>IS</given-names></name></person-group><article-title>Mutual information relevance networks: functional genomic clustering using pairwise entropy measurements</article-title><source>Pac Symp Biocomput</source><year>2000</year><volume>5</volume><fpage>427</fpage><lpage>439</lpage></citation></ref><ref id="B8"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Herwig</surname><given-names>R</given-names></name><name><surname>Poustka</surname><given-names>AJ</given-names></name><name><surname>Muller</surname><given-names>C</given-names></name><name><surname>Bull</surname><given-names>C</given-names></name><name><surname>Lehrach</surname><given-names>H</given-names></name><name><surname>O'brien</surname><given-names>J</given-names></name></person-group><article-title>Large-scale clustering of cDNA-fingerprinting data</article-title><source>Genome Res</source><year>1999</year><volume>9</volume><fpage>1093</fpage><lpage>1105</lpage><pub-id pub-id-type="pmid">10568749</pub-id><pub-id pub-id-type="doi">10.1101/gr.9.11.1093</pub-id></citation></ref><ref id="B9"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Korber</surname><given-names>BT</given-names></name><name><surname>Farber</surname><given-names>RM</given-names></name><name><surname>Wolpert</surname><given-names>DH</given-names></name><name><surname>Lapedes</surname><given-names>AS</given-names></name></person-group><article-title>Covariation of mutations in the V3 loop of human immunodeficiency virus type 1 envelope protein: An information theoretic analysis</article-title><source>Proc Natl Acad Sci USA</source><year>1993</year><volume>90</volume><fpage>7176</fpage><lpage>7180</lpage><pub-id pub-id-type="pmid">8346232</pub-id></citation></ref><ref id="B10"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Gorodkin</surname><given-names>J</given-names></name><name><surname>Heyer</surname><given-names>LJ</given-names></name><name><surname>Brunak</surname><given-names>S</given-names></name><name><surname>Stormo</surname><given-names>GD</given-names></name><name><surname>Wen</surname><given-names>X</given-names></name><name><surname>Somogyi</surname><given-names>R</given-names></name></person-group><article-title>Display the information contents of structural RNA alignments: the structure logos</article-title><source>Comput Appl Biosci</source><year>1997</year><volume>13</volume><fpage>583</fpage><lpage>586</lpage><pub-id pub-id-type="pmid">9475985</pub-id></citation></ref><ref id="B11"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Liang</surname><given-names>S</given-names></name><name><surname>Fuhrman</surname><given-names>S</given-names></name><name><surname>Somogyi</surname><given-names>R</given-names></name></person-group><article-title>Reveal, a general reverse engineering algorithm for inference of genetic network architectures</article-title><source>Pac Symp Biocomput</source><year>1998</year><fpage>18</fpage><lpage>29</lpage><pub-id pub-id-type="pmid">9697168</pub-id></citation></ref><ref id="B12"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Roberts</surname><given-names>S</given-names></name><name><surname>Everson</surname><given-names>R</given-names></name></person-group><source>Independent component analysis: Priciples and Practice</source><year>2001</year><publisher-name>Cambridge: Cambridge University Press</publisher-name></citation></ref><ref id="B13"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Hyv&#x000e4;rinen</surname><given-names>A</given-names></name><name><surname>Karhunne</surname><given-names>J</given-names></name><name><surname>Oja</surname><given-names>E</given-names></name></person-group><source>Independent component analysis</source><year>2001</year><publisher-name>New York: Wiley</publisher-name></citation></ref><ref id="B14"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Fraser</surname><given-names>AM</given-names></name><name><surname>Swinney</surname><given-names>HL</given-names></name></person-group><article-title>Independent coordinates for strange attractors from mutual information</article-title><source>Phys Rev A</source><year>1986</year><volume>33</volume><fpage>2318</fpage><lpage>2321</lpage><pub-id pub-id-type="doi">10.1103/PhysRevA.33.1134</pub-id></citation></ref><ref id="B15"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Th&#x000e9;nevaz</surname><given-names>P</given-names></name><name><surname>Unser</surname><given-names>M</given-names></name></person-group><article-title>Optimization of mutual information for multiresolution image registration</article-title><source>IEEE Trans Image Processing</source><year>2000</year><volume>9</volume><fpage>2083</fpage><lpage>2099</lpage><pub-id pub-id-type="doi">10.1109/83.887976</pub-id></citation></ref><ref id="B16"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Ellis</surname><given-names>DP</given-names></name><name><surname>Bilmes</surname><given-names>JA</given-names></name></person-group><article-title>Using mutual information to design feature combinations</article-title><source>In Proceedings of the International Conference on Spoken Language Processing: Beijing</source><ext-link ext-link-type="uri" xlink:href="http://www.icsi.berkeley.edu/ftp/global/pub/speech/papers/icslp00-cmi.pdf"/><comment>16&#x02013;20 October 2000</comment></citation></ref><ref id="B17"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Shannon</surname><given-names>CE</given-names></name></person-group><article-title>A mathematical theory of communication</article-title><source>The Bell System Technical Journal</source><year>1948</year><volume>27</volume><fpage>623</fpage><lpage>656</lpage></citation></ref><ref id="B18"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Moon</surname><given-names>Y</given-names></name><name><surname>Rajagopalan</surname><given-names>B</given-names></name><name><surname>Lall</surname><given-names>U</given-names></name></person-group><article-title>Estimation of mutual information using kernel density estimators</article-title><source>Phys Rev E</source><year>1995</year><volume>52</volume><fpage>2318</fpage><lpage>2321</lpage><pub-id pub-id-type="doi">10.1103/PhysRevE.52.2318</pub-id></citation></ref><ref id="B19"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Silverman</surname><given-names>BW</given-names></name></person-group><source>Density estimation for statistics and data analysis</source><year>1986</year><publisher-name>London: Chapman and Hall</publisher-name></citation></ref><ref id="B20"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Steuer</surname><given-names>R</given-names></name><name><surname>Kurths</surname><given-names>J</given-names></name><name><surname>Daub</surname><given-names>CO</given-names></name><name><surname>Weise</surname><given-names>J</given-names></name><name><surname>Selbig</surname><given-names>J</given-names></name></person-group><article-title>The mutual information: detecting end evaluating dependencies between variables</article-title><source>Bioinformatics</source><year>2002</year><fpage>S231</fpage><lpage>S240</lpage><pub-id pub-id-type="pmid">12386007</pub-id></citation></ref><ref id="B21"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Paninski</surname><given-names>L</given-names></name></person-group><article-title>Estimation of Entropy and Mutual Information</article-title><source>Neural Computation</source><year>2003</year><volume>15</volume><fpage>1191</fpage><lpage>1253</lpage><pub-id pub-id-type="doi">10.1162/089976603321780272</pub-id></citation></ref><ref id="B22"><citation citation-type="book"><person-group person-group-type="author"><name><surname>DeBoor</surname><given-names>C</given-names></name></person-group><source>A practical guide to splines</source><year>1978</year><publisher-name>New York: Springer</publisher-name></citation></ref><ref id="B23"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Unser</surname><given-names>M</given-names></name><name><surname>Aldroubi</surname><given-names>A</given-names></name><name><surname>Eden</surname><given-names>M</given-names></name></person-group><article-title>B-spline signal processing: Part 1 &#x02013; Theory</article-title><source>IEEE Trans Signal Precessing</source><year>1993</year><volume>41</volume><fpage>821</fpage><lpage>832</lpage><pub-id pub-id-type="doi">10.1109/78.193220</pub-id></citation></ref><ref id="B24"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Unser</surname><given-names>M</given-names></name><name><surname>Aldroubi</surname><given-names>A</given-names></name><name><surname>Eden</surname><given-names>M</given-names></name></person-group><article-title>B-spline signal processing: Part 2 &#x02013; Efficient design and applications</article-title><source>IEEE Trans Signal Precessing</source><year>1993</year><volume>41</volume><fpage>834</fpage><lpage>848</lpage><pub-id pub-id-type="doi">10.1109/78.193221</pub-id></citation></ref><ref id="B25"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Herzel</surname><given-names>H</given-names></name><name><surname>Schmidt</surname><given-names>AO</given-names></name><name><surname>Ebeling</surname><given-names>W</given-names></name></person-group><article-title>Finite sample effects in sequence analysis</article-title><source>Chaos, Solitons &#x00026; Fractals</source><year>1994</year><volume>4</volume><fpage>97</fpage><lpage>113</lpage><pub-id pub-id-type="doi">10.1016/0960-0779(94)90020-5</pub-id></citation></ref><ref id="B26"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Herzel</surname><given-names>H</given-names></name><name><surname>Grosse</surname><given-names>I</given-names></name></person-group><article-title>Measuring correlations in symbol sequences</article-title><source>Physica A</source><year>1995</year><volume>216</volume><fpage>518</fpage><lpage>542</lpage><pub-id pub-id-type="doi">10.1016/0378-4371(95)00104-F</pub-id></citation></ref><ref id="B27"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Grosse</surname><given-names>I</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Freund JA</surname></name></person-group><article-title>Estimating entropies from finite samples</article-title><source>In Dynamik, Evolution, Strukturen</source><year>1996</year><publisher-name>Berlin: Dr. K&#x000f6;ster</publisher-name><fpage>181</fpage><lpage>190</lpage></citation></ref><ref id="B28"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Roulston</surname><given-names>MS</given-names></name></person-group><article-title>Estimating the error on measured entropy and mutual information</article-title><source>Physica D</source><year>1999</year><volume>125</volume><fpage>285</fpage><lpage>294</lpage><pub-id pub-id-type="doi">10.1016/S0167-2789(98)00269-3</pub-id></citation></ref><ref id="B29"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Herzel</surname><given-names>H</given-names></name><name><surname>Grosse</surname><given-names>I</given-names></name></person-group><article-title>Correlations in DNA sequences: The role of protein coding segments</article-title><source>Phy Rev E</source><year>1997</year><volume>55</volume><fpage>800</fpage><lpage>810</lpage><pub-id pub-id-type="doi">10.1103/PhysRevE.55.800</pub-id></citation></ref><ref id="B30"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Klus</surname><given-names>GT</given-names></name><name><surname>Song</surname><given-names>A</given-names></name><name><surname>Schick</surname><given-names>A</given-names></name><name><surname>Wahde</surname><given-names>M</given-names></name><name><surname>Szallasi</surname><given-names>Z</given-names></name></person-group><article-title>Mutual Information Analysis as a Tool to Assess the Role of Aneuploidy in the Generation of Cancer-Associated Differential Gene Expression Patterns</article-title><source>Pac Symp Biocomput</source><year>2001</year><fpage>42</fpage><lpage>51</lpage><pub-id pub-id-type="pmid">11262960</pub-id></citation></ref><ref id="B31"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Hughes</surname><given-names>TR</given-names></name><name><surname>Marton</surname><given-names>MJ</given-names></name><name><surname>Jones</surname><given-names>AR</given-names></name><name><surname>Roberts</surname><given-names>CJ</given-names></name><name><surname>Stoughton</surname><given-names>R</given-names></name><name><surname>Armour</surname><given-names>CD</given-names></name><name><surname>Bennett</surname><given-names>HA</given-names></name><name><surname>Coffey</surname><given-names>HA</given-names></name><name><surname>Dai</surname><given-names>H</given-names></name><name><surname>He</surname><given-names>YD</given-names></name><name><surname>Kidd</surname><given-names>MJ</given-names></name><name><surname>King</surname><given-names>AM</given-names></name><name><surname>Meyer</surname><given-names>MR</given-names></name><name><surname>Slade</surname><given-names>D</given-names></name><name><surname>Lum</surname><given-names>PY</given-names></name><name><surname>Stepaniants</surname><given-names>SB</given-names></name><name><surname>Shoemaker</surname><given-names>DD</given-names></name><name><surname>Gachotte</surname><given-names>D</given-names></name><name><surname>Chakraburtty</surname><given-names>K</given-names></name><name><surname>Simon</surname><given-names>J</given-names></name><name><surname>Bard</surname><given-names>M</given-names></name><name><surname>Friend</surname><given-names>SH</given-names></name></person-group><article-title>Functional Discovery via a Compendium of Expression Profiles</article-title><source>Cell</source><year>2000</year><volume>102</volume><fpage>109</fpage><lpage>126</lpage><pub-id pub-id-type="pmid">10929718</pub-id><pub-id pub-id-type="doi">10.1016/S0092-8674(00)00015-5</pub-id></citation></ref><ref id="B32"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Steuer</surname><given-names>R</given-names></name><name><surname>Daub</surname><given-names>CO</given-names></name><name><surname>Selbig</surname><given-names>J</given-names></name><name><surname>Kurths</surname><given-names>J</given-names></name></person-group><article-title>Measuring distances between variables by mutual information</article-title><source>In Proceedings of the 27th Annual Conference of the Gesellschaft f&#x000fc;r Klassifikation: Cottbus</source><comment>12&#x02013;14 March 2003</comment></citation></ref><ref id="B33"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>He</surname><given-names>YD</given-names></name><name><surname>Dai</surname><given-names>H</given-names></name><name><surname>Schadt</surname><given-names>EE</given-names></name><name><surname>Cavet</surname><given-names>G</given-names></name><name><surname>Edwards</surname><given-names>SW</given-names></name><name><surname>Stepaniants</surname><given-names>SB</given-names></name><name><surname>Duenwald</surname><given-names>S</given-names></name><name><surname>Kleinhanz</surname><given-names>R</given-names></name><name><surname>Jones</surname><given-names>AR</given-names></name><name><surname>Shoemaker</surname><given-names>DD</given-names></name><name><surname>Stoughton</surname><given-names>RB</given-names></name></person-group><article-title>Microarray standard data set and figures of merit for comparing data processing methods and experiment design</article-title><source>Bioinformatics</source><year>2003</year><volume>19</volume><fpage>956</fpage><lpage>965</lpage><pub-id pub-id-type="pmid">12761058</pub-id><pub-id pub-id-type="doi">10.1093/bioinformatics/btg126</pub-id></citation></ref></ref-list><sec sec-type="display-objects"><title>Figures and Tables</title><fig position="float" id="F1"><label>Figure 1</label><caption><p>Two datasets <italic>X </italic>and <italic>Y </italic>(100 data points) show a hypothetical dependency <italic>f</italic>(<italic>x</italic>) = 4<italic>x</italic>(1 - <italic>x</italic>) (top). The Pearson correlation coefficient is not able to detect a significant correlation as shown in the histogram plot of the dataset compared to 300 realisations of shuffled data (left). Mutual information clearly shows that the two datasets are not statistically independent (right).</p></caption><graphic xlink:href="1471-2105-5-118-1"/></fig><fig position="float" id="F2"><label>Figure 2</label><caption><p>The continuous experimental data for the variable <italic>x </italic>needs to be binned for the calculation of mutual information. The indicator function of Eq. (5) counts the number of data points within each bin (example with <italic>M</italic><sub><italic>x </italic></sub>= 5 bins, left). The generalised indicator function based on B-spline functions of Eq. (8) extends the bins to polynomial functions (example with <italic>M</italic><sub><italic>x </italic></sub>= 5 bins and spline order <italic>k </italic>= 3, right). The bins now overlap and the weight of each data point to each of the bins is given by the value of the respective B-spline functions at the data point. By definition, all weights contributing to one data point sum up to unity.</p></caption><graphic xlink:href="1471-2105-5-118-2"/></fig><fig position="float" id="F3"><label>Figure 3</label><caption><p>Mutual information is estimated for artificially generated equidis-tributed random numbers from the simple binning (<italic>k </italic>= 1), the B-spline approach (<italic>k </italic>= 3), and the entropy estimator <inline-graphic xlink:href="1471-2105-5-118-i7.gif"/> using <italic>M </italic>= 6 bins, and additionally from the kernel density estimator. The average over an ensemble of 600 trials is shown as a function of the size of the dataset (left) together with the standard deviation (right).</p></caption><graphic xlink:href="1471-2105-5-118-3"/></fig><fig position="float" id="F4"><label>Figure 4</label><caption><p>Mutual information calculated for a dataset of 300 data points drawn from the distribution shown in Figure 1 (crosses). The number of bins was fixed to <italic>M </italic>= 6. The average mutual information for 300 shuffled realisations of the dataset is shown (circles) together with the standard deviation as error-bars. The largest value found within the ensemble of shuffled data is drawn as a dotted line (left). The significance was calculated from Eq. (13) (right).</p></caption><graphic xlink:href="1471-2105-5-118-4"/></fig><fig position="float" id="F5"><label>Figure 5</label><caption><p>Based on the distribution of Figure 1, the mutual information for 300 data points and two spline orders <italic>k </italic>= 1 and <italic>k </italic>= 3 is shown as a function of the number of bins <italic>M </italic>(crosses) together with mean (circles) and standard deviations (error-bars) of 300 surrogates. The dotted lines indicate the largest mutual information found within the ensemble of surrogate data.</p></caption><graphic xlink:href="1471-2105-5-118-5"/></fig><fig position="float" id="F6"><label>Figure 6</label><caption><p>The significance, <italic>S</italic>, as a function of the number of bins, <italic>M</italic>, for the two examples of Figure 5, and for the entropy estimator <inline-graphic xlink:href="1471-2105-5-118-i7.gif"/>. For kernel density estimators (KDE), the significance, which is not depending on <italic>M</italic>, is calculated to <italic>S </italic>= 92.</p></caption><graphic xlink:href="1471-2105-5-118-6"/></fig><fig position="float" id="F7"><label>Figure 7</label><caption><p>The Pearson correlation coefficient and the mutual information for all pairwise comparisons of genes for two large-scale gene expression datasets are shown (black points) overlayed by the same measures obtained from shuffled data (blue points). The expected mutual information calculated from Eq. (14) is shown as read curve. For the first dataset (left) genes containing undefined values were omitted resulting in 5345 genes measured under 300 experimental conditions [31]. For the second dataset (right) containing 22608 genes measured under 102 experimental conditions [33], a representative fraction is shown.</p></caption><graphic xlink:href="1471-2105-5-118-7"/></fig><fig position="float" id="F8"><label>Figure 8</label><caption><p>Examples of gene-gene plots for genes <italic>X </italic>and <italic>Y </italic>are shown for characteristic tuples (<italic>MI</italic>(<italic>X,Y</italic>), <italic>R</italic>(<italic>X,Y</italic>)) detected in Figure (7). For the first gene expression dataset under consideration [31], no non-linear correlations are detected. Moreover, tuples with high Pearson correlation and low mutual information, examples A and B, resulting from outlying values are detected. For the second dataset [33], however, tuples with low Pearson correlation and high mutual information are observed, see examples C and D. Such non-linear correlations are missed by solely using linear correlation measures.</p></caption><graphic xlink:href="1471-2105-5-118-8"/></fig><fig position="float" id="F9"><label>Figure 9</label><caption><p>Significance values for the two gene-gene comparisons shown in Figure 8, C and D (top and bottom, respectively) are calculated from 300 shuffled realisations based on the Pearson correlation coefficient (left) and the mutual information (right) as distance measures.</p></caption><graphic xlink:href="1471-2105-5-118-9"/></fig><table-wrap position="float" id="T1"><label>Table 1</label><caption><p>For the calculation of probabilities <italic>p</italic>(<italic>a<sub>i</sub></italic>) according to the B-spline approach, <italic>M</italic><sub><italic>x </italic></sub>weighting coefficients are determined for each value <italic>x</italic><sub><italic>u </italic></sub>of variable <italic>x</italic>.</p></caption><table frame="hsides" rules="groups"><thead><tr><td></td><td align="center"><italic>B</italic><sub><italic>i</italic></sub>=1,<italic>k</italic>=2(<italic>x</italic><sub><italic>u</italic></sub>)</td><td align="center"><italic>B</italic><sub><italic>i</italic></sub>=2,<italic>k</italic>=2(<italic>x</italic><sub><italic>u</italic></sub>)</td><td align="center"><italic>B</italic><sub><italic>i</italic></sub>=3,<italic>k</italic>=2(<italic>x</italic><sub><italic>u</italic></sub>)</td></tr></thead><tbody><tr><td align="center"><italic>x</italic><sub>1</sub></td><td align="center">1.0</td><td align="center">0.0</td><td align="center">0.0</td></tr><tr><td align="center"><italic>x</italic><sub>2</sub></td><td align="center">0.6</td><td align="center">0.4</td><td align="center">0.0</td></tr><tr><td align="center"><italic>x</italic><sub>3</sub></td><td align="center">0.2</td><td align="center">0.8</td><td align="center">0.0</td></tr><tr><td align="center"><italic>x</italic><sub>4</sub></td><td align="center">0.0</td><td align="center">0.8</td><td align="center">0.2</td></tr><tr><td align="center"><italic>x</italic><sub>5</sub></td><td align="center">0.0</td><td align="center">0.4</td><td align="center">0.6</td></tr><tr><td align="center"><italic>x</italic><sub>6</sub></td><td align="center">0.0</td><td align="center">0.0</td><td align="center">1.0</td></tr><tr><td align="center"><italic>p</italic>(<italic>a</italic><sub><italic>i</italic></sub>)</td><td align="center">1.8/6</td><td align="center">2.4/6</td><td align="center">1.8/6</td></tr></tbody></table></table-wrap></sec></back></article>



